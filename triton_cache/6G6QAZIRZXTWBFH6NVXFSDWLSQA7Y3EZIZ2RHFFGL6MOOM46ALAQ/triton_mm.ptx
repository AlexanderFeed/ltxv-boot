//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_86
.address_size 64

	// .globl	triton_mm               // -- Begin function triton_mm
.extern .shared .align 16 .b8 global_smem[];
                                        // @triton_mm
.visible .entry triton_mm(
	.param .u64 .ptr .global .align 1 triton_mm_param_0,
	.param .u64 .ptr .global .align 1 triton_mm_param_1,
	.param .u64 .ptr .global .align 1 triton_mm_param_2,
	.param .u64 .ptr .global .align 1 triton_mm_param_3
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<134>;
	.reg .b32 	%r<3104>;
	.reg .b64 	%rd<172>;
	.loc	1 17 0                          // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:17:0
$L__func_begin0:
	.loc	1 17 0                          // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:17:0

// %bb.0:
	ld.param.u64 	%rd17, [triton_mm_param_2];
	ld.param.u64 	%rd43, [triton_mm_param_0];
	ld.param.u64 	%rd44, [triton_mm_param_1];
$L__tmp0:
	.loc	1 40 24                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:40:24
	mov.u32 	%r348, %ctaid.x;
	.loc	1 46 22                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:46:22
	mul.hi.s32 	%r349, %r348, 715827883;
	shr.u32 	%r350, %r349, 31;
	shr.s32 	%r351, %r349, 6;
	add.s32 	%r352, %r351, %r350;
	.loc	1 47 41                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:47:41
	shl.b32 	%r353, %r352, 3;
	.loc	1 47 30                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:47:30
	sub.s32 	%r354, 33, %r353;
	.loc	1 47 50                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:47:50
	min.s32 	%r355, %r354, 8;
	.loc	1 48 40                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:48:40
	rem.s32 	%r356, %r348, %r355;
	.loc	1 48 34                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:48:34
	add.s32 	%r357, %r356, %r353;
	.loc	1 49 19                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:49:19
	mul.lo.s32 	%r358, %r352, 384;
	sub.s32 	%r359, %r348, %r358;
	.loc	1 49 30                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:49:30
	div.s32 	%r360, %r359, %r355;
	.loc	1 51 17                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:51:17
	shl.b32 	%r1, %r357, 7;
	.loc	1 51 40                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:51:40
	mov.u32 	%r2, %tid.x;
	and.b32  	%r361, %r2, 8;
	and.b32  	%r3, %r2, 16;
	and.b32  	%r362, %r2, 32;
	bfe.u32 	%r363, %r2, 3, 4;
	and.b32  	%r4, %r2, 128;
	shr.u32 	%r364, %r4, 3;
	or.b32  	%r365, %r363, %r364;
	or.b32  	%r366, %r365, 32;
	or.b32  	%r367, %r365, 64;
	or.b32  	%r368, %r365, 96;
	.loc	1 51 27                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:51:27
	or.b32  	%r369, %r1, %r365;
	or.b32  	%r370, %r1, %r366;
	or.b32  	%r371, %r1, %r367;
	or.b32  	%r372, %r1, %r368;
	.loc	1 52 17                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:52:17
	shl.b32 	%r5, %r360, 8;
	.loc	1 52 27                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:52:27
	or.b32  	%r373, %r5, %r365;
	or.b32  	%r374, %r5, %r366;
	or.b32  	%r375, %r5, %r367;
	or.b32  	%r376, %r5, %r368;
	or.b32  	%r377, %r373, 128;
	or.b32  	%r378, %r373, 160;
	or.b32  	%r379, %r373, 192;
	or.b32  	%r380, %r373, 224;
	.loc	1 54 57                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:54:57
	mul.hi.s32 	%r381, %r369, 2139127681;
	shr.u32 	%r382, %r381, 31;
	shr.s32 	%r383, %r381, 11;
	add.s32 	%r384, %r383, %r382;
	mul.lo.s32 	%r385, %r384, 4112;
	sub.s32 	%r386, %r369, %r385;
	mul.hi.s32 	%r387, %r370, 2139127681;
	shr.u32 	%r388, %r387, 31;
	shr.s32 	%r389, %r387, 11;
	add.s32 	%r390, %r389, %r388;
	mul.lo.s32 	%r391, %r390, 4112;
	sub.s32 	%r392, %r370, %r391;
	mul.hi.s32 	%r393, %r371, 2139127681;
	shr.u32 	%r394, %r393, 31;
	shr.s32 	%r395, %r393, 11;
	add.s32 	%r396, %r395, %r394;
	mul.lo.s32 	%r397, %r396, 4112;
	sub.s32 	%r398, %r371, %r397;
	mul.hi.s32 	%r399, %r372, 2139127681;
	shr.u32 	%r400, %r399, 31;
	shr.s32 	%r401, %r399, 11;
	add.s32 	%r402, %r401, %r400;
	mul.lo.s32 	%r403, %r402, 4112;
	sub.s32 	%r404, %r372, %r403;
	.loc	1 58 57                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:58:57
	mul.hi.s32 	%r405, %r373, 715827883;
	shr.u32 	%r406, %r405, 31;
	shr.u32 	%r407, %r405, 11;
	add.s32 	%r408, %r407, %r406;
	mul.lo.s32 	%r409, %r408, 12288;
	sub.s32 	%r410, %r373, %r409;
	mul.hi.s32 	%r411, %r374, 715827883;
	shr.u32 	%r412, %r411, 31;
	shr.u32 	%r413, %r411, 11;
	add.s32 	%r414, %r413, %r412;
	mul.lo.s32 	%r415, %r414, 12288;
	sub.s32 	%r416, %r374, %r415;
	mul.hi.s32 	%r417, %r375, 715827883;
	shr.u32 	%r418, %r417, 31;
	shr.u32 	%r419, %r417, 11;
	add.s32 	%r420, %r419, %r418;
	mul.lo.s32 	%r421, %r420, 12288;
	sub.s32 	%r422, %r375, %r421;
	mul.hi.s32 	%r423, %r376, 715827883;
	shr.u32 	%r424, %r423, 31;
	shr.u32 	%r425, %r423, 11;
	add.s32 	%r426, %r425, %r424;
	mul.lo.s32 	%r427, %r426, 12288;
	sub.s32 	%r428, %r376, %r427;
	mul.hi.s32 	%r429, %r377, 715827883;
	shr.u32 	%r430, %r429, 31;
	shr.u32 	%r431, %r429, 11;
	add.s32 	%r432, %r431, %r430;
	mul.lo.s32 	%r433, %r432, 12288;
	sub.s32 	%r434, %r377, %r433;
	mul.hi.s32 	%r435, %r378, 715827883;
	shr.u32 	%r436, %r435, 31;
	shr.u32 	%r437, %r435, 11;
	add.s32 	%r438, %r437, %r436;
	mul.lo.s32 	%r439, %r438, 12288;
	sub.s32 	%r440, %r378, %r439;
	mul.hi.s32 	%r441, %r379, 715827883;
	shr.u32 	%r442, %r441, 31;
	shr.u32 	%r443, %r441, 11;
	add.s32 	%r444, %r443, %r442;
	mul.lo.s32 	%r445, %r444, 12288;
	sub.s32 	%r446, %r379, %r445;
	mul.hi.s32 	%r447, %r380, 715827883;
	shr.u32 	%r448, %r447, 31;
	shr.u32 	%r449, %r447, 11;
	add.s32 	%r450, %r449, %r448;
	mul.lo.s32 	%r451, %r450, 12288;
	sub.s32 	%r452, %r380, %r451;
	.loc	1 66 30                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:66:30
	shl.b32 	%r453, %r2, 4;
	and.b32  	%r454, %r453, 16;
	and.b32  	%r455, %r453, 32;
	and.b32  	%r456, %r453, 48;
	and.b32  	%r457, %r453, 64;
	and.b32  	%r458, %r453, 112;
	.loc	1 71 30                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:71:30
	mul.lo.s32 	%r459, %r386, 3072;
	mul.lo.s32 	%r460, %r392, 3072;
	mul.lo.s32 	%r461, %r398, 3072;
	mul.lo.s32 	%r462, %r404, 3072;
	.loc	1 77 55                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:55
	mul.lo.s32 	%r463, %r410, 3072;
	mul.lo.s32 	%r464, %r416, 3072;
	mul.lo.s32 	%r465, %r422, 3072;
	mul.lo.s32 	%r466, %r428, 3072;
	mul.lo.s32 	%r467, %r434, 3072;
	mul.lo.s32 	%r468, %r440, 3072;
	mul.lo.s32 	%r469, %r446, 3072;
	mul.lo.s32 	%r470, %r452, 3072;
	.loc	1 71 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:71:25
	or.b32  	%r471, %r459, %r458;
	or.b32  	%r472, %r460, %r458;
	or.b32  	%r473, %r461, %r458;
	or.b32  	%r474, %r462, %r458;
	.loc	1 72 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:25
	cvt.s64.s32 	%rd45, %r471;
	add.s64 	%rd18, %rd43, %rd45;
	cvt.s64.s32 	%rd46, %r472;
	add.s64 	%rd19, %rd43, %rd46;
	cvt.s64.s32 	%rd47, %r473;
	add.s64 	%rd20, %rd43, %rd47;
	cvt.s64.s32 	%rd48, %r474;
	add.s64 	%rd21, %rd43, %rd48;
	.loc	1 72 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:20
	shl.b32 	%r6, %r2, 1;
	and.b32  	%r475, %r6, 48;
	xor.b32  	%r7, %r475, %r458;
	shl.b32 	%r476, %r362, 1;
	xor.b32  	%r477, %r7, %r476;
	shl.b32 	%r478, %r365, 7;
	or.b32  	%r9, %r477, %r478;
	mov.u32 	%r479, global_smem;
	add.s32 	%r305, %r479, %r9;
	add.s32 	%r297, %r305, 65536;
	add.s32 	%r299, %r305, 69632;
	add.s32 	%r301, %r305, 73728;
	add.s32 	%r303, %r305, 77824;
	mov.b32 	%r298, 16;
	// begin inline asm
	cp.async.cg.shared.global [ %r297 + 0 ], [ %rd18 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r299 + 0 ], [ %rd19 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r301 + 0 ], [ %rd20 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r303 + 0 ], [ %rd21 + 0 ], 0x10, %r298;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 50                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:50
	or.b32  	%r480, %r463, %r458;
	or.b32  	%r481, %r464, %r458;
	or.b32  	%r482, %r465, %r458;
	or.b32  	%r483, %r466, %r458;
	or.b32  	%r484, %r467, %r458;
	or.b32  	%r485, %r468, %r458;
	or.b32  	%r486, %r469, %r458;
	or.b32  	%r487, %r470, %r458;
	.loc	1 77 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:25
	cvt.s64.s32 	%rd49, %r480;
	add.s64 	%rd22, %rd44, %rd49;
	cvt.s64.s32 	%rd50, %r481;
	add.s64 	%rd23, %rd44, %rd50;
	cvt.s64.s32 	%rd51, %r482;
	add.s64 	%rd24, %rd44, %rd51;
	cvt.s64.s32 	%rd52, %r483;
	add.s64 	%rd25, %rd44, %rd52;
	cvt.s64.s32 	%rd53, %r484;
	add.s64 	%rd26, %rd44, %rd53;
	cvt.s64.s32 	%rd54, %r485;
	add.s64 	%rd27, %rd44, %rd54;
	cvt.s64.s32 	%rd55, %r486;
	add.s64 	%rd28, %rd44, %rd55;
	cvt.s64.s32 	%rd56, %r487;
	add.s64 	%rd29, %rd44, %rd56;
	.loc	1 77 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:20
	add.s32 	%r307, %r305, 4096;
	add.s32 	%r309, %r305, 8192;
	add.s32 	%r311, %r305, 12288;
	add.s32 	%r313, %r305, 16384;
	add.s32 	%r315, %r305, 20480;
	add.s32 	%r317, %r305, 24576;
	add.s32 	%r319, %r305, 28672;
	// begin inline asm
	cp.async.cg.shared.global [ %r305 + 0 ], [ %rd22 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r307 + 0 ], [ %rd23 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r309 + 0 ], [ %rd24 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r311 + 0 ], [ %rd25 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r313 + 0 ], [ %rd26 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r315 + 0 ], [ %rd27 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r317 + 0 ], [ %rd28 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r319 + 0 ], [ %rd29 + 0 ], 0x10, %r298;
	// end inline asm
	cp.async.commit_group;
	.loc	1 72 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:25
	cvt.s64.s32 	%rd57, %r459;
	cvt.u64.u32 	%rd58, %r458;
	or.b64  	%rd59, %rd57, %rd58;
	add.s64 	%rd60, %rd43, %rd59;
	add.s64 	%rd30, %rd60, 128;
	cvt.s64.s32 	%rd61, %r460;
	or.b64  	%rd62, %rd61, %rd58;
	add.s64 	%rd63, %rd43, %rd62;
	add.s64 	%rd31, %rd63, 128;
	cvt.s64.s32 	%rd64, %r461;
	or.b64  	%rd65, %rd64, %rd58;
	add.s64 	%rd66, %rd43, %rd65;
	add.s64 	%rd32, %rd66, 128;
	cvt.s64.s32 	%rd67, %r462;
	or.b64  	%rd68, %rd67, %rd58;
	add.s64 	%rd69, %rd43, %rd68;
	add.s64 	%rd33, %rd69, 128;
	.loc	1 72 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:20
	bar.sync 	0;
	add.s32 	%r321, %r305, 81920;
	add.s32 	%r323, %r305, 86016;
	add.s32 	%r325, %r305, 90112;
	add.s32 	%r327, %r305, 94208;
	// begin inline asm
	cp.async.cg.shared.global [ %r321 + 0 ], [ %rd30 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r323 + 0 ], [ %rd31 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r325 + 0 ], [ %rd32 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r327 + 0 ], [ %rd33 + 0 ], 0x10, %r298;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:25
	cvt.s64.s32 	%rd70, %r463;
	or.b64  	%rd71, %rd70, %rd58;
	add.s64 	%rd72, %rd44, %rd71;
	add.s64 	%rd34, %rd72, 128;
	cvt.s64.s32 	%rd73, %r464;
	or.b64  	%rd74, %rd73, %rd58;
	add.s64 	%rd75, %rd44, %rd74;
	add.s64 	%rd35, %rd75, 128;
	cvt.s64.s32 	%rd76, %r465;
	or.b64  	%rd77, %rd76, %rd58;
	add.s64 	%rd78, %rd44, %rd77;
	add.s64 	%rd36, %rd78, 128;
	cvt.s64.s32 	%rd79, %r466;
	or.b64  	%rd80, %rd79, %rd58;
	add.s64 	%rd81, %rd44, %rd80;
	add.s64 	%rd37, %rd81, 128;
	cvt.s64.s32 	%rd82, %r467;
	or.b64  	%rd83, %rd82, %rd58;
	add.s64 	%rd84, %rd44, %rd83;
	add.s64 	%rd38, %rd84, 128;
	cvt.s64.s32 	%rd85, %r468;
	or.b64  	%rd86, %rd85, %rd58;
	add.s64 	%rd87, %rd44, %rd86;
	add.s64 	%rd39, %rd87, 128;
	cvt.s64.s32 	%rd88, %r469;
	or.b64  	%rd89, %rd88, %rd58;
	add.s64 	%rd90, %rd44, %rd89;
	add.s64 	%rd40, %rd90, 128;
	cvt.s64.s32 	%rd91, %r470;
	or.b64  	%rd92, %rd91, %rd58;
	add.s64 	%rd93, %rd44, %rd92;
	add.s64 	%rd41, %rd93, 128;
	.loc	1 77 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:20
	add.s32 	%r329, %r305, 32768;
	add.s32 	%r331, %r305, 36864;
	add.s32 	%r333, %r305, 40960;
	add.s32 	%r335, %r305, 45056;
	add.s32 	%r337, %r305, 49152;
	add.s32 	%r339, %r305, 53248;
	add.s32 	%r341, %r305, 57344;
	add.s32 	%r343, %r305, 61440;
	// begin inline asm
	cp.async.cg.shared.global [ %r329 + 0 ], [ %rd34 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r331 + 0 ], [ %rd35 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r333 + 0 ], [ %rd36 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r335 + 0 ], [ %rd37 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r337 + 0 ], [ %rd38 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r339 + 0 ], [ %rd39 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r341 + 0 ], [ %rd40 + 0 ], 0x10, %r298;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r343 + 0 ], [ %rd41 + 0 ], 0x10, %r298;
	// end inline asm
	cp.async.commit_group;
	.loc	1 64 26                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:64:26
	or.b32  	%r488, %r457, %r3;
	or.b32  	%r489, %r361, %r364;
	and.b32  	%r490, %r2, 7;
	or.b32  	%r491, %r489, %r490;
	and.b32  	%r492, %r2, 15;
	xor.b32  	%r493, %r458, %r3;
	or.b32  	%r494, %r364, %r492;
	shl.b32 	%r495, %r494, 7;
	or.b32  	%r10, %r495, %r493;
	or.b32  	%r496, %r454, 32;
	xor.b32  	%r497, %r496, %r455;
	or.b32  	%r498, %r497, %r457;
	xor.b32  	%r499, %r498, %r3;
	or.b32  	%r11, %r499, %r495;
	or.b32  	%r500, %r456, 64;
	xor.b32  	%r501, %r500, %r488;
	or.b32  	%r12, %r501, %r495;
	or.b32  	%r502, %r454, 96;
	and.b32  	%r503, %r453, 96;
	or.b32  	%r504, %r503, %r3;
	xor.b32  	%r505, %r504, %r502;
	or.b32  	%r13, %r505, %r495;
	shl.b32 	%r506, %r491, 7;
	or.b32  	%r507, %r506, 4096;
	or.b32  	%r14, %r507, %r493;
	or.b32  	%r15, %r499, %r507;
	or.b32  	%r16, %r501, %r507;
	or.b32  	%r17, %r505, %r507;
	or.b32  	%r508, %r506, 8192;
	or.b32  	%r18, %r508, %r493;
	or.b32  	%r19, %r499, %r508;
	or.b32  	%r20, %r501, %r508;
	or.b32  	%r21, %r505, %r508;
	shr.u32 	%r509, %r362, 2;
	or.b32  	%r510, %r490, %r509;
	or.b32  	%r511, %r506, 12288;
	or.b32  	%r22, %r511, %r493;
	or.b32  	%r23, %r499, %r511;
	or.b32  	%r24, %r501, %r511;
	or.b32  	%r25, %r505, %r511;
	shr.u32 	%r26, %r2, 2;
	and.b32  	%r512, %r26, 16;
	or.b32  	%r513, %r512, %r509;
	or.b32  	%r514, %r513, %r490;
	shl.b32 	%r27, %r514, 7;
	or.b32  	%r28, %r27, %r7;
	or.b32  	%r515, %r457, %r475;
	xor.b32  	%r29, %r515, %r500;
	or.b32  	%r30, %r29, %r27;
	or.b32  	%r516, %r510, %r512;
	shl.b32 	%r517, %r516, 7;
	or.b32  	%r518, %r517, 4096;
	or.b32  	%r31, %r518, %r7;
	or.b32  	%r32, %r29, %r518;
	or.b32  	%r519, %r517, 8192;
	or.b32  	%r33, %r519, %r7;
	or.b32  	%r34, %r29, %r519;
	or.b32  	%r520, %r517, 12288;
	or.b32  	%r35, %r520, %r7;
	or.b32  	%r36, %r29, %r520;
	add.s64 	%rd1, %rd93, 256;
	add.s64 	%rd2, %rd90, 256;
	add.s64 	%rd3, %rd87, 256;
	add.s64 	%rd4, %rd84, 256;
	add.s64 	%rd5, %rd81, 256;
	add.s64 	%rd6, %rd78, 256;
	add.s64 	%rd7, %rd75, 256;
	add.s64 	%rd8, %rd72, 256;
	add.s64 	%rd9, %rd69, 256;
	add.s64 	%rd10, %rd66, 256;
	add.s64 	%rd11, %rd63, 256;
	add.s64 	%rd12, %rd60, 256;
	mov.b32 	%r2976, 0;
	mov.b32 	%r2975, 1;
	mov.b32 	%r2974, -1;
	mov.b64 	%rd170, 0;
	mov.u64 	%rd171, %rd170;
	mov.u32 	%r2977, %r2976;
	mov.u32 	%r2978, %r2976;
	mov.u32 	%r2979, %r2976;
	mov.u32 	%r2980, %r2976;
	mov.u32 	%r2981, %r2976;
	mov.u32 	%r2982, %r2976;
	mov.u32 	%r2983, %r2976;
	mov.u32 	%r2984, %r2976;
	mov.u32 	%r2985, %r2976;
	mov.u32 	%r2986, %r2976;
	mov.u32 	%r2987, %r2976;
	mov.u32 	%r2988, %r2976;
	mov.u32 	%r2989, %r2976;
	mov.u32 	%r2990, %r2976;
	mov.u32 	%r2991, %r2976;
	mov.u32 	%r2992, %r2976;
	mov.u32 	%r2993, %r2976;
	mov.u32 	%r2994, %r2976;
	mov.u32 	%r2995, %r2976;
	mov.u32 	%r2996, %r2976;
	mov.u32 	%r2997, %r2976;
	mov.u32 	%r2998, %r2976;
	mov.u32 	%r2999, %r2976;
	mov.u32 	%r3000, %r2976;
	mov.u32 	%r3001, %r2976;
	mov.u32 	%r3002, %r2976;
	mov.u32 	%r3003, %r2976;
	mov.u32 	%r3004, %r2976;
	mov.u32 	%r3005, %r2976;
	mov.u32 	%r3006, %r2976;
	mov.u32 	%r3007, %r2976;
	mov.u32 	%r3008, %r2976;
	mov.u32 	%r3009, %r2976;
	mov.u32 	%r3010, %r2976;
	mov.u32 	%r3011, %r2976;
	mov.u32 	%r3012, %r2976;
	mov.u32 	%r3013, %r2976;
	mov.u32 	%r3014, %r2976;
	mov.u32 	%r3015, %r2976;
	mov.u32 	%r3016, %r2976;
	mov.u32 	%r3017, %r2976;
	mov.u32 	%r3018, %r2976;
	mov.u32 	%r3019, %r2976;
	mov.u32 	%r3020, %r2976;
	mov.u32 	%r3021, %r2976;
	mov.u32 	%r3022, %r2976;
	mov.u32 	%r3023, %r2976;
	mov.u32 	%r3024, %r2976;
	mov.u32 	%r3025, %r2976;
	mov.u32 	%r3026, %r2976;
	mov.u32 	%r3027, %r2976;
	mov.u32 	%r3028, %r2976;
	mov.u32 	%r3029, %r2976;
	mov.u32 	%r3030, %r2976;
	mov.u32 	%r3031, %r2976;
	mov.u32 	%r3032, %r2976;
	mov.u32 	%r3033, %r2976;
	mov.u32 	%r3034, %r2976;
	mov.u32 	%r3035, %r2976;
	mov.u32 	%r3036, %r2976;
	mov.u32 	%r3037, %r2976;
	mov.u32 	%r3038, %r2976;
	mov.u32 	%r3039, %r2976;
	mov.u32 	%r3040, %r2976;
	mov.u32 	%r3041, %r2976;
	mov.u32 	%r3042, %r2976;
	mov.u32 	%r3043, %r2976;
	mov.u32 	%r3044, %r2976;
	mov.u32 	%r3045, %r2976;
	mov.u32 	%r3046, %r2976;
	mov.u32 	%r3047, %r2976;
	mov.u32 	%r3048, %r2976;
	mov.u32 	%r3049, %r2976;
	mov.u32 	%r3050, %r2976;
	mov.u32 	%r3051, %r2976;
	mov.u32 	%r3052, %r2976;
	mov.u32 	%r3053, %r2976;
	mov.u32 	%r3054, %r2976;
	mov.u32 	%r3055, %r2976;
	mov.u32 	%r3056, %r2976;
	mov.u32 	%r3057, %r2976;
	mov.u32 	%r3058, %r2976;
	mov.u32 	%r3059, %r2976;
	mov.u32 	%r3060, %r2976;
	mov.u32 	%r3061, %r2976;
	mov.u32 	%r3062, %r2976;
	mov.u32 	%r3063, %r2976;
	mov.u32 	%r3064, %r2976;
	mov.u32 	%r3065, %r2976;
	mov.u32 	%r3066, %r2976;
	mov.u32 	%r3067, %r2976;
	mov.u32 	%r3068, %r2976;
	mov.u32 	%r3069, %r2976;
	mov.u32 	%r3070, %r2976;
	mov.u32 	%r3071, %r2976;
	mov.u32 	%r3072, %r2976;
	mov.u32 	%r3073, %r2976;
	mov.u32 	%r3074, %r2976;
	mov.u32 	%r3075, %r2976;
	mov.u32 	%r3076, %r2976;
	mov.u32 	%r3077, %r2976;
	mov.u32 	%r3078, %r2976;
	mov.u32 	%r3079, %r2976;
	mov.u32 	%r3080, %r2976;
	mov.u32 	%r3081, %r2976;
	mov.u32 	%r3082, %r2976;
	mov.u32 	%r3083, %r2976;
	mov.u32 	%r3084, %r2976;
	mov.u32 	%r3085, %r2976;
	mov.u32 	%r3086, %r2976;
	mov.u32 	%r3087, %r2976;
	mov.u32 	%r3088, %r2976;
	mov.u32 	%r3089, %r2976;
	mov.u32 	%r3090, %r2976;
	mov.u32 	%r3091, %r2976;
	mov.u32 	%r3092, %r2976;
	mov.u32 	%r3093, %r2976;
	mov.u32 	%r3094, %r2976;
	mov.u32 	%r3095, %r2976;
	mov.u32 	%r3096, %r2976;
	mov.u32 	%r3097, %r2976;
	mov.u32 	%r3098, %r2976;
	mov.u32 	%r3099, %r2976;
	mov.u32 	%r3100, %r2976;
	mov.u32 	%r3101, %r2976;
	mov.u32 	%r3102, %r2976;
	mov.u32 	%r3103, %r2976;
$L__BB0_1:                              // =>This Inner Loop Header: Depth=1
	setp.lt.u64 	%p1, %rd171, 22;
	add.s32 	%r2497, %r2974, 1;
	setp.lt.s32 	%p2, %r2497, 2;
	selp.b32 	%r2974, %r2497, 0, %p2;
	.loc	1 72 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:20
	cp.async.wait_group 2;
	bar.sync 	0;
	shl.b32 	%r2498, %r2974, 14;
	add.s32 	%r2500, %r479, 65536;
	add.s32 	%r2501, %r2500, %r2498;
	add.s32 	%r525, %r2501, %r10;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r689, %r690, %r691, %r692}, [%r525];
	// end inline asm
	add.s32 	%r530, %r2501, %r11;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1137, %r1138, %r1139, %r1140}, [%r530];
	// end inline asm
	add.s32 	%r535, %r2501, %r12;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1585, %r1586, %r1587, %r1588}, [%r535];
	// end inline asm
	add.s32 	%r540, %r2501, %r13;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r2033, %r2034, %r2035, %r2036}, [%r540];
	// end inline asm
	add.s32 	%r545, %r2501, %r14;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r801, %r802, %r803, %r804}, [%r545];
	// end inline asm
	add.s32 	%r550, %r2501, %r15;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1249, %r1250, %r1251, %r1252}, [%r550];
	// end inline asm
	add.s32 	%r555, %r2501, %r16;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1697, %r1698, %r1699, %r1700}, [%r555];
	// end inline asm
	add.s32 	%r560, %r2501, %r17;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r2145, %r2146, %r2147, %r2148}, [%r560];
	// end inline asm
	add.s32 	%r565, %r2501, %r18;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r913, %r914, %r915, %r916}, [%r565];
	// end inline asm
	add.s32 	%r570, %r2501, %r19;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1361, %r1362, %r1363, %r1364}, [%r570];
	// end inline asm
	add.s32 	%r575, %r2501, %r20;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1809, %r1810, %r1811, %r1812}, [%r575];
	// end inline asm
	add.s32 	%r580, %r2501, %r21;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r2257, %r2258, %r2259, %r2260}, [%r580];
	// end inline asm
	add.s32 	%r585, %r2501, %r22;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1025, %r1026, %r1027, %r1028}, [%r585];
	// end inline asm
	add.s32 	%r590, %r2501, %r23;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1473, %r1474, %r1475, %r1476}, [%r590];
	// end inline asm
	add.s32 	%r595, %r2501, %r24;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1921, %r1922, %r1923, %r1924}, [%r595];
	// end inline asm
	add.s32 	%r600, %r2501, %r25;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r2369, %r2370, %r2371, %r2372}, [%r600];
	// end inline asm
	.loc	1 77 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:20
	shl.b32 	%r2502, %r2974, 15;
	add.s32 	%r2503, %r479, %r2502;
	add.s32 	%r605, %r2503, %r28;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r693, %r694, %r1141, %r1142}, [%r605];
	// end inline asm
	add.s32 	%r610, %r2503, %r30;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1589, %r1590, %r2037, %r2038}, [%r610];
	// end inline asm
	add.s32 	%r615, %r2503, %r31;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r707, %r708, %r1155, %r1156}, [%r615];
	// end inline asm
	add.s32 	%r620, %r2503, %r32;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1603, %r1604, %r2051, %r2052}, [%r620];
	// end inline asm
	add.s32 	%r625, %r2503, %r33;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r721, %r722, %r1169, %r1170}, [%r625];
	// end inline asm
	add.s32 	%r630, %r2503, %r34;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1617, %r1618, %r2065, %r2066}, [%r630];
	// end inline asm
	add.s32 	%r635, %r2503, %r35;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r735, %r736, %r1183, %r1184}, [%r635];
	// end inline asm
	add.s32 	%r640, %r2503, %r36;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1631, %r1632, %r2079, %r2080}, [%r640];
	// end inline asm
	add.s32 	%r2504, %r27, %r7;
	add.s32 	%r2505, %r2503, %r2504;
	add.s32 	%r645, %r2505, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r749, %r750, %r1197, %r1198}, [%r645];
	// end inline asm
	add.s32 	%r2506, %r29, %r27;
	add.s32 	%r2507, %r2503, %r2506;
	add.s32 	%r650, %r2507, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1645, %r1646, %r2093, %r2094}, [%r650];
	// end inline asm
	add.s32 	%r655, %r2505, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r763, %r764, %r1211, %r1212}, [%r655];
	// end inline asm
	add.s32 	%r660, %r2507, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1659, %r1660, %r2107, %r2108}, [%r660];
	// end inline asm
	add.s32 	%r665, %r2505, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r777, %r778, %r1225, %r1226}, [%r665];
	// end inline asm
	add.s32 	%r670, %r2507, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1673, %r1674, %r2121, %r2122}, [%r670];
	// end inline asm
	add.s32 	%r675, %r2505, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r791, %r792, %r1239, %r1240}, [%r675];
	// end inline asm
	add.s32 	%r680, %r2507, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1687, %r1688, %r2135, %r2136}, [%r680];
	// end inline asm
	.loc	1 78 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:78:25
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2976, %r2977, %r2978, %r2979 }, { %r689, %r690, %r691, %r692 }, { %r693, %r694 }, { %r2976, %r2977, %r2978, %r2979 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2980, %r2981, %r2982, %r2983 }, { %r689, %r690, %r691, %r692 }, { %r707, %r708 }, { %r2980, %r2981, %r2982, %r2983 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2984, %r2985, %r2986, %r2987 }, { %r689, %r690, %r691, %r692 }, { %r721, %r722 }, { %r2984, %r2985, %r2986, %r2987 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2988, %r2989, %r2990, %r2991 }, { %r689, %r690, %r691, %r692 }, { %r735, %r736 }, { %r2988, %r2989, %r2990, %r2991 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2992, %r2993, %r2994, %r2995 }, { %r689, %r690, %r691, %r692 }, { %r749, %r750 }, { %r2992, %r2993, %r2994, %r2995 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2996, %r2997, %r2998, %r2999 }, { %r689, %r690, %r691, %r692 }, { %r763, %r764 }, { %r2996, %r2997, %r2998, %r2999 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3000, %r3001, %r3002, %r3003 }, { %r689, %r690, %r691, %r692 }, { %r777, %r778 }, { %r3000, %r3001, %r3002, %r3003 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3004, %r3005, %r3006, %r3007 }, { %r689, %r690, %r691, %r692 }, { %r791, %r792 }, { %r3004, %r3005, %r3006, %r3007 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3008, %r3009, %r3010, %r3011 }, { %r801, %r802, %r803, %r804 }, { %r693, %r694 }, { %r3008, %r3009, %r3010, %r3011 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3012, %r3013, %r3014, %r3015 }, { %r801, %r802, %r803, %r804 }, { %r707, %r708 }, { %r3012, %r3013, %r3014, %r3015 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3016, %r3017, %r3018, %r3019 }, { %r801, %r802, %r803, %r804 }, { %r721, %r722 }, { %r3016, %r3017, %r3018, %r3019 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3020, %r3021, %r3022, %r3023 }, { %r801, %r802, %r803, %r804 }, { %r735, %r736 }, { %r3020, %r3021, %r3022, %r3023 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3024, %r3025, %r3026, %r3027 }, { %r801, %r802, %r803, %r804 }, { %r749, %r750 }, { %r3024, %r3025, %r3026, %r3027 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3028, %r3029, %r3030, %r3031 }, { %r801, %r802, %r803, %r804 }, { %r763, %r764 }, { %r3028, %r3029, %r3030, %r3031 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3032, %r3033, %r3034, %r3035 }, { %r801, %r802, %r803, %r804 }, { %r777, %r778 }, { %r3032, %r3033, %r3034, %r3035 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3036, %r3037, %r3038, %r3039 }, { %r801, %r802, %r803, %r804 }, { %r791, %r792 }, { %r3036, %r3037, %r3038, %r3039 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3040, %r3041, %r3042, %r3043 }, { %r913, %r914, %r915, %r916 }, { %r693, %r694 }, { %r3040, %r3041, %r3042, %r3043 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3044, %r3045, %r3046, %r3047 }, { %r913, %r914, %r915, %r916 }, { %r707, %r708 }, { %r3044, %r3045, %r3046, %r3047 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3048, %r3049, %r3050, %r3051 }, { %r913, %r914, %r915, %r916 }, { %r721, %r722 }, { %r3048, %r3049, %r3050, %r3051 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3052, %r3053, %r3054, %r3055 }, { %r913, %r914, %r915, %r916 }, { %r735, %r736 }, { %r3052, %r3053, %r3054, %r3055 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3056, %r3057, %r3058, %r3059 }, { %r913, %r914, %r915, %r916 }, { %r749, %r750 }, { %r3056, %r3057, %r3058, %r3059 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3060, %r3061, %r3062, %r3063 }, { %r913, %r914, %r915, %r916 }, { %r763, %r764 }, { %r3060, %r3061, %r3062, %r3063 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3064, %r3065, %r3066, %r3067 }, { %r913, %r914, %r915, %r916 }, { %r777, %r778 }, { %r3064, %r3065, %r3066, %r3067 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3068, %r3069, %r3070, %r3071 }, { %r913, %r914, %r915, %r916 }, { %r791, %r792 }, { %r3068, %r3069, %r3070, %r3071 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3072, %r3073, %r3074, %r3075 }, { %r1025, %r1026, %r1027, %r1028 }, { %r693, %r694 }, { %r3072, %r3073, %r3074, %r3075 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3076, %r3077, %r3078, %r3079 }, { %r1025, %r1026, %r1027, %r1028 }, { %r707, %r708 }, { %r3076, %r3077, %r3078, %r3079 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3080, %r3081, %r3082, %r3083 }, { %r1025, %r1026, %r1027, %r1028 }, { %r721, %r722 }, { %r3080, %r3081, %r3082, %r3083 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3084, %r3085, %r3086, %r3087 }, { %r1025, %r1026, %r1027, %r1028 }, { %r735, %r736 }, { %r3084, %r3085, %r3086, %r3087 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3088, %r3089, %r3090, %r3091 }, { %r1025, %r1026, %r1027, %r1028 }, { %r749, %r750 }, { %r3088, %r3089, %r3090, %r3091 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3092, %r3093, %r3094, %r3095 }, { %r1025, %r1026, %r1027, %r1028 }, { %r763, %r764 }, { %r3092, %r3093, %r3094, %r3095 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3096, %r3097, %r3098, %r3099 }, { %r1025, %r1026, %r1027, %r1028 }, { %r777, %r778 }, { %r3096, %r3097, %r3098, %r3099 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3100, %r3101, %r3102, %r3103 }, { %r1025, %r1026, %r1027, %r1028 }, { %r791, %r792 }, { %r3100, %r3101, %r3102, %r3103 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2976, %r2977, %r2978, %r2979 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1141, %r1142 }, { %r2976, %r2977, %r2978, %r2979 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2980, %r2981, %r2982, %r2983 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1155, %r1156 }, { %r2980, %r2981, %r2982, %r2983 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2984, %r2985, %r2986, %r2987 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1169, %r1170 }, { %r2984, %r2985, %r2986, %r2987 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2988, %r2989, %r2990, %r2991 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1183, %r1184 }, { %r2988, %r2989, %r2990, %r2991 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2992, %r2993, %r2994, %r2995 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1197, %r1198 }, { %r2992, %r2993, %r2994, %r2995 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2996, %r2997, %r2998, %r2999 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1211, %r1212 }, { %r2996, %r2997, %r2998, %r2999 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3000, %r3001, %r3002, %r3003 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1225, %r1226 }, { %r3000, %r3001, %r3002, %r3003 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3004, %r3005, %r3006, %r3007 }, { %r1137, %r1138, %r1139, %r1140 }, { %r1239, %r1240 }, { %r3004, %r3005, %r3006, %r3007 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3008, %r3009, %r3010, %r3011 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1141, %r1142 }, { %r3008, %r3009, %r3010, %r3011 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3012, %r3013, %r3014, %r3015 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1155, %r1156 }, { %r3012, %r3013, %r3014, %r3015 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3016, %r3017, %r3018, %r3019 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1169, %r1170 }, { %r3016, %r3017, %r3018, %r3019 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3020, %r3021, %r3022, %r3023 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1183, %r1184 }, { %r3020, %r3021, %r3022, %r3023 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3024, %r3025, %r3026, %r3027 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1197, %r1198 }, { %r3024, %r3025, %r3026, %r3027 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3028, %r3029, %r3030, %r3031 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1211, %r1212 }, { %r3028, %r3029, %r3030, %r3031 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3032, %r3033, %r3034, %r3035 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1225, %r1226 }, { %r3032, %r3033, %r3034, %r3035 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3036, %r3037, %r3038, %r3039 }, { %r1249, %r1250, %r1251, %r1252 }, { %r1239, %r1240 }, { %r3036, %r3037, %r3038, %r3039 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3040, %r3041, %r3042, %r3043 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1141, %r1142 }, { %r3040, %r3041, %r3042, %r3043 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3044, %r3045, %r3046, %r3047 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1155, %r1156 }, { %r3044, %r3045, %r3046, %r3047 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3048, %r3049, %r3050, %r3051 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1169, %r1170 }, { %r3048, %r3049, %r3050, %r3051 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3052, %r3053, %r3054, %r3055 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1183, %r1184 }, { %r3052, %r3053, %r3054, %r3055 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3056, %r3057, %r3058, %r3059 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1197, %r1198 }, { %r3056, %r3057, %r3058, %r3059 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3060, %r3061, %r3062, %r3063 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1211, %r1212 }, { %r3060, %r3061, %r3062, %r3063 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3064, %r3065, %r3066, %r3067 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1225, %r1226 }, { %r3064, %r3065, %r3066, %r3067 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3068, %r3069, %r3070, %r3071 }, { %r1361, %r1362, %r1363, %r1364 }, { %r1239, %r1240 }, { %r3068, %r3069, %r3070, %r3071 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3072, %r3073, %r3074, %r3075 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1141, %r1142 }, { %r3072, %r3073, %r3074, %r3075 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3076, %r3077, %r3078, %r3079 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1155, %r1156 }, { %r3076, %r3077, %r3078, %r3079 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3080, %r3081, %r3082, %r3083 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1169, %r1170 }, { %r3080, %r3081, %r3082, %r3083 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3084, %r3085, %r3086, %r3087 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1183, %r1184 }, { %r3084, %r3085, %r3086, %r3087 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3088, %r3089, %r3090, %r3091 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1197, %r1198 }, { %r3088, %r3089, %r3090, %r3091 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3092, %r3093, %r3094, %r3095 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1211, %r1212 }, { %r3092, %r3093, %r3094, %r3095 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3096, %r3097, %r3098, %r3099 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1225, %r1226 }, { %r3096, %r3097, %r3098, %r3099 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3100, %r3101, %r3102, %r3103 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1239, %r1240 }, { %r3100, %r3101, %r3102, %r3103 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2976, %r2977, %r2978, %r2979 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1589, %r1590 }, { %r2976, %r2977, %r2978, %r2979 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2980, %r2981, %r2982, %r2983 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1603, %r1604 }, { %r2980, %r2981, %r2982, %r2983 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2984, %r2985, %r2986, %r2987 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1617, %r1618 }, { %r2984, %r2985, %r2986, %r2987 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2988, %r2989, %r2990, %r2991 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1631, %r1632 }, { %r2988, %r2989, %r2990, %r2991 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2992, %r2993, %r2994, %r2995 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1645, %r1646 }, { %r2992, %r2993, %r2994, %r2995 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2996, %r2997, %r2998, %r2999 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1659, %r1660 }, { %r2996, %r2997, %r2998, %r2999 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3000, %r3001, %r3002, %r3003 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1673, %r1674 }, { %r3000, %r3001, %r3002, %r3003 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3004, %r3005, %r3006, %r3007 }, { %r1585, %r1586, %r1587, %r1588 }, { %r1687, %r1688 }, { %r3004, %r3005, %r3006, %r3007 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3008, %r3009, %r3010, %r3011 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1589, %r1590 }, { %r3008, %r3009, %r3010, %r3011 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3012, %r3013, %r3014, %r3015 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1603, %r1604 }, { %r3012, %r3013, %r3014, %r3015 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3016, %r3017, %r3018, %r3019 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1617, %r1618 }, { %r3016, %r3017, %r3018, %r3019 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3020, %r3021, %r3022, %r3023 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1631, %r1632 }, { %r3020, %r3021, %r3022, %r3023 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3024, %r3025, %r3026, %r3027 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1645, %r1646 }, { %r3024, %r3025, %r3026, %r3027 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3028, %r3029, %r3030, %r3031 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1659, %r1660 }, { %r3028, %r3029, %r3030, %r3031 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3032, %r3033, %r3034, %r3035 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1673, %r1674 }, { %r3032, %r3033, %r3034, %r3035 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3036, %r3037, %r3038, %r3039 }, { %r1697, %r1698, %r1699, %r1700 }, { %r1687, %r1688 }, { %r3036, %r3037, %r3038, %r3039 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3040, %r3041, %r3042, %r3043 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1589, %r1590 }, { %r3040, %r3041, %r3042, %r3043 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3044, %r3045, %r3046, %r3047 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1603, %r1604 }, { %r3044, %r3045, %r3046, %r3047 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3048, %r3049, %r3050, %r3051 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1617, %r1618 }, { %r3048, %r3049, %r3050, %r3051 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3052, %r3053, %r3054, %r3055 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1631, %r1632 }, { %r3052, %r3053, %r3054, %r3055 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3056, %r3057, %r3058, %r3059 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1645, %r1646 }, { %r3056, %r3057, %r3058, %r3059 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3060, %r3061, %r3062, %r3063 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1659, %r1660 }, { %r3060, %r3061, %r3062, %r3063 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3064, %r3065, %r3066, %r3067 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1673, %r1674 }, { %r3064, %r3065, %r3066, %r3067 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3068, %r3069, %r3070, %r3071 }, { %r1809, %r1810, %r1811, %r1812 }, { %r1687, %r1688 }, { %r3068, %r3069, %r3070, %r3071 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3072, %r3073, %r3074, %r3075 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1589, %r1590 }, { %r3072, %r3073, %r3074, %r3075 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3076, %r3077, %r3078, %r3079 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1603, %r1604 }, { %r3076, %r3077, %r3078, %r3079 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3080, %r3081, %r3082, %r3083 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1617, %r1618 }, { %r3080, %r3081, %r3082, %r3083 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3084, %r3085, %r3086, %r3087 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1631, %r1632 }, { %r3084, %r3085, %r3086, %r3087 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3088, %r3089, %r3090, %r3091 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1645, %r1646 }, { %r3088, %r3089, %r3090, %r3091 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3092, %r3093, %r3094, %r3095 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1659, %r1660 }, { %r3092, %r3093, %r3094, %r3095 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3096, %r3097, %r3098, %r3099 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1673, %r1674 }, { %r3096, %r3097, %r3098, %r3099 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3100, %r3101, %r3102, %r3103 }, { %r1921, %r1922, %r1923, %r1924 }, { %r1687, %r1688 }, { %r3100, %r3101, %r3102, %r3103 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2976, %r2977, %r2978, %r2979 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2037, %r2038 }, { %r2976, %r2977, %r2978, %r2979 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2980, %r2981, %r2982, %r2983 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2051, %r2052 }, { %r2980, %r2981, %r2982, %r2983 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2984, %r2985, %r2986, %r2987 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2065, %r2066 }, { %r2984, %r2985, %r2986, %r2987 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2988, %r2989, %r2990, %r2991 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2079, %r2080 }, { %r2988, %r2989, %r2990, %r2991 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2992, %r2993, %r2994, %r2995 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2093, %r2094 }, { %r2992, %r2993, %r2994, %r2995 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r2996, %r2997, %r2998, %r2999 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2107, %r2108 }, { %r2996, %r2997, %r2998, %r2999 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3000, %r3001, %r3002, %r3003 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2121, %r2122 }, { %r3000, %r3001, %r3002, %r3003 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3004, %r3005, %r3006, %r3007 }, { %r2033, %r2034, %r2035, %r2036 }, { %r2135, %r2136 }, { %r3004, %r3005, %r3006, %r3007 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3008, %r3009, %r3010, %r3011 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2037, %r2038 }, { %r3008, %r3009, %r3010, %r3011 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3012, %r3013, %r3014, %r3015 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2051, %r2052 }, { %r3012, %r3013, %r3014, %r3015 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3016, %r3017, %r3018, %r3019 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2065, %r2066 }, { %r3016, %r3017, %r3018, %r3019 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3020, %r3021, %r3022, %r3023 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2079, %r2080 }, { %r3020, %r3021, %r3022, %r3023 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3024, %r3025, %r3026, %r3027 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2093, %r2094 }, { %r3024, %r3025, %r3026, %r3027 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3028, %r3029, %r3030, %r3031 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2107, %r2108 }, { %r3028, %r3029, %r3030, %r3031 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3032, %r3033, %r3034, %r3035 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2121, %r2122 }, { %r3032, %r3033, %r3034, %r3035 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3036, %r3037, %r3038, %r3039 }, { %r2145, %r2146, %r2147, %r2148 }, { %r2135, %r2136 }, { %r3036, %r3037, %r3038, %r3039 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3040, %r3041, %r3042, %r3043 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2037, %r2038 }, { %r3040, %r3041, %r3042, %r3043 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3044, %r3045, %r3046, %r3047 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2051, %r2052 }, { %r3044, %r3045, %r3046, %r3047 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3048, %r3049, %r3050, %r3051 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2065, %r2066 }, { %r3048, %r3049, %r3050, %r3051 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3052, %r3053, %r3054, %r3055 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2079, %r2080 }, { %r3052, %r3053, %r3054, %r3055 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3056, %r3057, %r3058, %r3059 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2093, %r2094 }, { %r3056, %r3057, %r3058, %r3059 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3060, %r3061, %r3062, %r3063 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2107, %r2108 }, { %r3060, %r3061, %r3062, %r3063 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3064, %r3065, %r3066, %r3067 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2121, %r2122 }, { %r3064, %r3065, %r3066, %r3067 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3068, %r3069, %r3070, %r3071 }, { %r2257, %r2258, %r2259, %r2260 }, { %r2135, %r2136 }, { %r3068, %r3069, %r3070, %r3071 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3072, %r3073, %r3074, %r3075 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2037, %r2038 }, { %r3072, %r3073, %r3074, %r3075 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3076, %r3077, %r3078, %r3079 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2051, %r2052 }, { %r3076, %r3077, %r3078, %r3079 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3080, %r3081, %r3082, %r3083 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2065, %r2066 }, { %r3080, %r3081, %r3082, %r3083 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3084, %r3085, %r3086, %r3087 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2079, %r2080 }, { %r3084, %r3085, %r3086, %r3087 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3088, %r3089, %r3090, %r3091 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2093, %r2094 }, { %r3088, %r3089, %r3090, %r3091 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3092, %r3093, %r3094, %r3095 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2107, %r2108 }, { %r3092, %r3093, %r3094, %r3095 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3096, %r3097, %r3098, %r3099 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2121, %r2122 }, { %r3096, %r3097, %r3098, %r3099 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 { %r3100, %r3101, %r3102, %r3103 }, { %r2369, %r2370, %r2371, %r2372 }, { %r2135, %r2136 }, { %r3100, %r3101, %r3102, %r3103 };
	// end inline asm
	.loc	1 64 26                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:64:26
	add.s32 	%r2508, %r2975, 1;
	setp.lt.s32 	%p3, %r2508, 2;
	selp.b32 	%r2975, %r2508, 0, %p3;
	.loc	1 72 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:25
	add.s64 	%rd94, %rd12, %rd170;
	add.s64 	%rd95, %rd11, %rd170;
	add.s64 	%rd96, %rd10, %rd170;
	.loc	1 72 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:72:20
	add.s64 	%rd97, %rd9, %rd170;
	shl.b32 	%r2509, %r2975, 14;
	add.s32 	%r2510, %r2500, %r2509;
	bar.sync 	0;
	add.s32 	%r2473, %r2510, %r9;
	add.s32 	%r2475, %r2473, 4096;
	add.s32 	%r2477, %r2473, 8192;
	add.s32 	%r2479, %r2473, 12288;
	selp.b32 	%r2474, 16, 0, %p1;
	// begin inline asm
	cp.async.cg.shared.global [ %r2473 + 0 ], [ %rd94 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2475 + 0 ], [ %rd95 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2477 + 0 ], [ %rd96 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2479 + 0 ], [ %rd97 + 0 ], 0x10, %r2474;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:25
	add.s64 	%rd98, %rd8, %rd170;
	add.s64 	%rd99, %rd7, %rd170;
	add.s64 	%rd100, %rd6, %rd170;
	add.s64 	%rd101, %rd5, %rd170;
	add.s64 	%rd102, %rd4, %rd170;
	add.s64 	%rd103, %rd3, %rd170;
	add.s64 	%rd104, %rd2, %rd170;
	.loc	1 77 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:77:20
	add.s64 	%rd105, %rd1, %rd170;
	shl.b32 	%r2512, %r2975, 15;
	add.s32 	%r2513, %r479, %r2512;
	add.s32 	%r2481, %r2513, %r9;
	add.s32 	%r2483, %r2481, 4096;
	add.s32 	%r2485, %r2481, 8192;
	add.s32 	%r2487, %r2481, 12288;
	add.s32 	%r2489, %r2481, 16384;
	add.s32 	%r2491, %r2481, 20480;
	add.s32 	%r2493, %r2481, 24576;
	add.s32 	%r2495, %r2481, 28672;
	// begin inline asm
	cp.async.cg.shared.global [ %r2481 + 0 ], [ %rd98 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2483 + 0 ], [ %rd99 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2485 + 0 ], [ %rd100 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2487 + 0 ], [ %rd101 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2489 + 0 ], [ %rd102 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2491 + 0 ], [ %rd103 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2493 + 0 ], [ %rd104 + 0 ], 0x10, %r2474;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r2495 + 0 ], [ %rd105 + 0 ], 0x10, %r2474;
	// end inline asm
	cp.async.commit_group;
	.loc	1 64 26                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:64:26
	add.s64 	%rd171, %rd171, 1;
	add.s64 	%rd170, %rd170, 128;
	setp.ne.s64 	%p4, %rd170, 3072;
	@%p4 bra 	$L__BB0_1;
// %bb.2:
	.loc	1 52 40                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:52:40
	shl.b32 	%r2835, %r2, 2;
	and.b32  	%r2836, %r2835, 252;
	.loc	1 52 27                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:52:27
	or.b32  	%r2837, %r5, %r2836;
	.loc	1 51 40                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:51:40
	bfe.u32 	%r2838, %r2, 6, 2;
	.loc	1 51 27                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:51:27
	or.b32  	%r2839, %r2838, %r1;
	or.b32  	%r2840, %r2839, 124;
	or.b32  	%r2841, %r2839, 120;
	or.b32  	%r2842, %r2839, 116;
	or.b32  	%r2843, %r2839, 112;
	or.b32  	%r2844, %r2839, 108;
	or.b32  	%r2845, %r2839, 104;
	or.b32  	%r2846, %r2839, 100;
	or.b32  	%r2847, %r2839, 96;
	or.b32  	%r2848, %r2839, 92;
	or.b32  	%r2849, %r2839, 88;
	or.b32  	%r2850, %r2839, 84;
	or.b32  	%r2851, %r2839, 80;
	or.b32  	%r2852, %r2839, 76;
	or.b32  	%r2853, %r2839, 72;
	or.b32  	%r2854, %r2839, 68;
	or.b32  	%r2855, %r2839, 64;
	or.b32  	%r2856, %r2839, 60;
	or.b32  	%r2857, %r2839, 56;
	or.b32  	%r2858, %r2839, 52;
	or.b32  	%r2859, %r2839, 48;
	or.b32  	%r2860, %r2839, 44;
	or.b32  	%r2861, %r2839, 40;
	or.b32  	%r2862, %r2839, 36;
	or.b32  	%r2863, %r2839, 32;
	or.b32  	%r2864, %r2839, 28;
	or.b32  	%r2865, %r2839, 24;
	or.b32  	%r2866, %r2839, 20;
	or.b32  	%r2867, %r2839, 16;
	or.b32  	%r2868, %r2839, 12;
	or.b32  	%r2869, %r2839, 8;
	or.b32  	%r2870, %r2839, 4;
	.loc	1 64 26                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:64:26
	cp.async.wait_group 0;
	bar.sync 	0;
	.loc	1 85 20                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:85:20
	setp.lt.s32 	%p101, %r2839, 4112;
	setp.lt.s32 	%p102, %r2870, 4112;
	setp.lt.s32 	%p103, %r2869, 4112;
	setp.lt.s32 	%p104, %r2868, 4112;
	setp.lt.s32 	%p105, %r2867, 4112;
	setp.lt.s32 	%p106, %r2866, 4112;
	setp.lt.s32 	%p107, %r2865, 4112;
	setp.lt.s32 	%p108, %r2864, 4112;
	setp.lt.s32 	%p109, %r2863, 4112;
	setp.lt.s32 	%p110, %r2862, 4112;
	setp.lt.s32 	%p111, %r2861, 4112;
	setp.lt.s32 	%p112, %r2860, 4112;
	setp.lt.s32 	%p113, %r2859, 4112;
	setp.lt.s32 	%p114, %r2858, 4112;
	setp.lt.s32 	%p115, %r2857, 4112;
	setp.lt.s32 	%p116, %r2856, 4112;
	setp.lt.s32 	%p117, %r2855, 4112;
	setp.lt.s32 	%p118, %r2854, 4112;
	setp.lt.s32 	%p119, %r2853, 4112;
	setp.lt.s32 	%p120, %r2852, 4112;
	setp.lt.s32 	%p121, %r2851, 4112;
	setp.lt.s32 	%p122, %r2850, 4112;
	setp.lt.s32 	%p123, %r2849, 4112;
	setp.lt.s32 	%p124, %r2848, 4112;
	setp.lt.s32 	%p125, %r2847, 4112;
	setp.lt.s32 	%p126, %r2846, 4112;
	setp.lt.s32 	%p127, %r2845, 4112;
	setp.lt.s32 	%p128, %r2844, 4112;
	setp.lt.s32 	%p129, %r2843, 4112;
	setp.lt.s32 	%p130, %r2842, 4112;
	setp.lt.s32 	%p131, %r2841, 4112;
	setp.lt.s32 	%p132, %r2840, 4112;
	.loc	1 85 34                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:85:34
	setp.lt.s32 	%p133, %r2837, 12288;
	.loc	1 85 26                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:85:26
	and.pred  	%p69, %p101, %p133;
	and.pred  	%p70, %p102, %p133;
	and.pred  	%p71, %p103, %p133;
	and.pred  	%p72, %p104, %p133;
	and.pred  	%p73, %p105, %p133;
	and.pred  	%p74, %p106, %p133;
	and.pred  	%p75, %p107, %p133;
	and.pred  	%p76, %p108, %p133;
	and.pred  	%p77, %p109, %p133;
	and.pred  	%p78, %p110, %p133;
	and.pred  	%p79, %p111, %p133;
	and.pred  	%p80, %p112, %p133;
	and.pred  	%p81, %p113, %p133;
	and.pred  	%p82, %p114, %p133;
	and.pred  	%p83, %p115, %p133;
	and.pred  	%p84, %p116, %p133;
	and.pred  	%p85, %p117, %p133;
	and.pred  	%p86, %p118, %p133;
	and.pred  	%p87, %p119, %p133;
	and.pred  	%p88, %p120, %p133;
	and.pred  	%p89, %p121, %p133;
	and.pred  	%p90, %p122, %p133;
	and.pred  	%p91, %p123, %p133;
	and.pred  	%p92, %p124, %p133;
	and.pred  	%p93, %p125, %p133;
	and.pred  	%p94, %p126, %p133;
	and.pred  	%p95, %p127, %p133;
	and.pred  	%p96, %p128, %p133;
	and.pred  	%p97, %p129, %p133;
	and.pred  	%p98, %p130, %p133;
	and.pred  	%p99, %p131, %p133;
	and.pred  	%p100, %p132, %p133;
	.loc	1 88 27                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:88:27
	mad.lo.s32 	%r2871, %r2839, 12288, %r2837;
	add.s32 	%r2872, %r2871, 49152;
	add.s32 	%r2873, %r2871, 98304;
	add.s32 	%r2874, %r2871, 147456;
	add.s32 	%r2875, %r2871, 196608;
	add.s32 	%r2876, %r2871, 245760;
	add.s32 	%r2877, %r2871, 294912;
	add.s32 	%r2878, %r2871, 344064;
	add.s32 	%r2879, %r2871, 393216;
	add.s32 	%r2880, %r2871, 442368;
	add.s32 	%r2881, %r2871, 491520;
	add.s32 	%r2882, %r2871, 540672;
	add.s32 	%r2883, %r2871, 589824;
	add.s32 	%r2884, %r2871, 638976;
	add.s32 	%r2885, %r2871, 688128;
	add.s32 	%r2886, %r2871, 737280;
	add.s32 	%r2887, %r2871, 786432;
	add.s32 	%r2888, %r2871, 835584;
	add.s32 	%r2889, %r2871, 884736;
	add.s32 	%r2890, %r2871, 933888;
	add.s32 	%r2891, %r2871, 983040;
	add.s32 	%r2892, %r2871, 1032192;
	add.s32 	%r2893, %r2871, 1081344;
	add.s32 	%r2894, %r2871, 1130496;
	add.s32 	%r2895, %r2871, 1179648;
	add.s32 	%r2896, %r2871, 1228800;
	add.s32 	%r2897, %r2871, 1277952;
	add.s32 	%r2898, %r2871, 1327104;
	add.s32 	%r2899, %r2871, 1376256;
	add.s32 	%r2900, %r2871, 1425408;
	add.s32 	%r2901, %r2871, 1474560;
	.loc	1 88 21                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:88:21
	add.s32 	%r2902, %r2871, 1523712;
	.loc	1 89 25                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:89:25
	mul.wide.s32 	%rd138, %r2871, 4;
	add.s64 	%rd106, %rd17, %rd138;
	mul.wide.s32 	%rd139, %r2872, 4;
	add.s64 	%rd107, %rd17, %rd139;
	mul.wide.s32 	%rd140, %r2873, 4;
	add.s64 	%rd108, %rd17, %rd140;
	mul.wide.s32 	%rd141, %r2874, 4;
	add.s64 	%rd109, %rd17, %rd141;
	mul.wide.s32 	%rd142, %r2875, 4;
	add.s64 	%rd110, %rd17, %rd142;
	mul.wide.s32 	%rd143, %r2876, 4;
	add.s64 	%rd111, %rd17, %rd143;
	mul.wide.s32 	%rd144, %r2877, 4;
	add.s64 	%rd112, %rd17, %rd144;
	mul.wide.s32 	%rd145, %r2878, 4;
	add.s64 	%rd113, %rd17, %rd145;
	mul.wide.s32 	%rd146, %r2879, 4;
	add.s64 	%rd114, %rd17, %rd146;
	mul.wide.s32 	%rd147, %r2880, 4;
	add.s64 	%rd115, %rd17, %rd147;
	mul.wide.s32 	%rd148, %r2881, 4;
	add.s64 	%rd116, %rd17, %rd148;
	mul.wide.s32 	%rd149, %r2882, 4;
	add.s64 	%rd117, %rd17, %rd149;
	mul.wide.s32 	%rd150, %r2883, 4;
	add.s64 	%rd118, %rd17, %rd150;
	mul.wide.s32 	%rd151, %r2884, 4;
	add.s64 	%rd119, %rd17, %rd151;
	mul.wide.s32 	%rd152, %r2885, 4;
	add.s64 	%rd120, %rd17, %rd152;
	mul.wide.s32 	%rd153, %r2886, 4;
	add.s64 	%rd121, %rd17, %rd153;
	mul.wide.s32 	%rd154, %r2887, 4;
	add.s64 	%rd122, %rd17, %rd154;
	mul.wide.s32 	%rd155, %r2888, 4;
	add.s64 	%rd123, %rd17, %rd155;
	mul.wide.s32 	%rd156, %r2889, 4;
	add.s64 	%rd124, %rd17, %rd156;
	mul.wide.s32 	%rd157, %r2890, 4;
	add.s64 	%rd125, %rd17, %rd157;
	mul.wide.s32 	%rd158, %r2891, 4;
	add.s64 	%rd126, %rd17, %rd158;
	mul.wide.s32 	%rd159, %r2892, 4;
	add.s64 	%rd127, %rd17, %rd159;
	mul.wide.s32 	%rd160, %r2893, 4;
	add.s64 	%rd128, %rd17, %rd160;
	mul.wide.s32 	%rd161, %r2894, 4;
	add.s64 	%rd129, %rd17, %rd161;
	mul.wide.s32 	%rd162, %r2895, 4;
	add.s64 	%rd130, %rd17, %rd162;
	mul.wide.s32 	%rd163, %r2896, 4;
	add.s64 	%rd131, %rd17, %rd163;
	mul.wide.s32 	%rd164, %r2897, 4;
	add.s64 	%rd132, %rd17, %rd164;
	mul.wide.s32 	%rd165, %r2898, 4;
	add.s64 	%rd133, %rd17, %rd165;
	mul.wide.s32 	%rd166, %r2899, 4;
	add.s64 	%rd134, %rd17, %rd166;
	mul.wide.s32 	%rd167, %r2900, 4;
	add.s64 	%rd135, %rd17, %rd167;
	mul.wide.s32 	%rd168, %r2901, 4;
	add.s64 	%rd136, %rd17, %rd168;
	mul.wide.s32 	%rd169, %r2902, 4;
	add.s64 	%rd137, %rd17, %rd169;
	.loc	1 89 67                         // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:89:67
	and.b32  	%r2903, %r6, 6;
	shl.b32 	%r2904, %r2, 6;
	and.b32  	%r2905, %r2904, 768;
	or.b32  	%r2906, %r2905, %r2903;
	shl.b32 	%r2907, %r3, 6;
	or.b32  	%r2908, %r2906, %r2907;
	and.b32  	%r2909, %r26, 24;
	or.b32  	%r2910, %r2908, %r2909;
	shl.b32 	%r2911, %r4, 5;
	or.b32  	%r2912, %r2910, %r2911;
	and.b32  	%r2913, %r2835, 1020;
	shr.u32 	%r2914, %r2912, 6;
	add.s32 	%r2915, %r2914, %r2912;
	shl.b32 	%r2916, %r2915, 2;
	add.s32 	%r2515, %r479, %r2916;
	mov.pred 	%p5, -1;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2515 + 0 ], { %r2976, %r2977 };
	// end inline asm
	or.b32  	%r2918, %r2912, 2048;
	shr.u32 	%r2919, %r2918, 6;
	add.s32 	%r2920, %r2919, %r2912;
	shl.b32 	%r2921, %r2920, 2;
	add.s32 	%r2922, %r479, %r2921;
	add.s32 	%r2518, %r2922, 8192;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2518 + 0 ], { %r2978, %r2979 };
	// end inline asm
	add.s32 	%r2521, %r2515, 128;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2521 + 0 ], { %r2980, %r2981 };
	// end inline asm
	add.s32 	%r2524, %r2922, 8320;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2524 + 0 ], { %r2982, %r2983 };
	// end inline asm
	add.s32 	%r2527, %r2515, 256;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2527 + 0 ], { %r2984, %r2985 };
	// end inline asm
	and.b32  	%r2923, %r2919, 67108860;
	add.s32 	%r2924, %r2923, %r2912;
	shl.b32 	%r2925, %r2924, 2;
	add.s32 	%r2926, %r479, %r2925;
	add.s32 	%r2530, %r2926, 8448;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2530 + 0 ], { %r2986, %r2987 };
	// end inline asm
	add.s32 	%r2533, %r2515, 384;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2533 + 0 ], { %r2988, %r2989 };
	// end inline asm
	add.s32 	%r2536, %r2926, 8576;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2536 + 0 ], { %r2990, %r2991 };
	// end inline asm
	add.s32 	%r2539, %r2515, 512;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2539 + 0 ], { %r2992, %r2993 };
	// end inline asm
	add.s32 	%r2542, %r2926, 8704;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2542 + 0 ], { %r2994, %r2995 };
	// end inline asm
	add.s32 	%r2545, %r2515, 640;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2545 + 0 ], { %r2996, %r2997 };
	// end inline asm
	add.s32 	%r2548, %r2926, 8832;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2548 + 0 ], { %r2998, %r2999 };
	// end inline asm
	add.s32 	%r2551, %r2515, 768;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2551 + 0 ], { %r3000, %r3001 };
	// end inline asm
	add.s32 	%r2554, %r2926, 8960;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2554 + 0 ], { %r3002, %r3003 };
	// end inline asm
	add.s32 	%r2557, %r2515, 896;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2557 + 0 ], { %r3004, %r3005 };
	// end inline asm
	add.s32 	%r2560, %r2926, 9088;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2560 + 0 ], { %r3006, %r3007 };
	// end inline asm
	bar.sync 	0;
	shr.u32 	%r2927, %r2, 4;
	and.b32  	%r2928, %r2927, 12;
	add.s32 	%r2929, %r2928, %r2913;
	shl.b32 	%r2930, %r2929, 2;
	add.s32 	%r2931, %r479, %r2930;
	ld.shared.v4.u32 	{%r2707, %r2708, %r2709, %r2710}, [%r2931];
	or.b32  	%r2932, %r2913, 1024;
	shr.u32 	%r2933, %r2932, 6;
	and.b32  	%r2934, %r2933, 28;
	add.s32 	%r2935, %r2934, %r2913;
	shl.b32 	%r2936, %r2935, 2;
	add.s32 	%r2937, %r479, %r2936;
	ld.shared.v4.u32 	{%r2711, %r2712, %r2713, %r2714}, [%r2937+4096];
	or.b32  	%r2938, %r2913, 2048;
	shr.u32 	%r2939, %r2938, 6;
	and.b32  	%r2940, %r2939, 44;
	add.s32 	%r2941, %r2940, %r2913;
	shl.b32 	%r2942, %r2941, 2;
	add.s32 	%r2943, %r479, %r2942;
	ld.shared.v4.u32 	{%r2715, %r2716, %r2717, %r2718}, [%r2943+8192];
	or.b32  	%r2944, %r2913, 3072;
	shr.u32 	%r2945, %r2944, 6;
	and.b32  	%r2946, %r2945, 60;
	add.s32 	%r2947, %r2946, %r2913;
	shl.b32 	%r2948, %r2947, 2;
	add.s32 	%r2949, %r479, %r2948;
	ld.shared.v4.u32 	{%r2719, %r2720, %r2721, %r2722}, [%r2949+12288];
	or.b32  	%r2950, %r2913, 4096;
	shr.u32 	%r2951, %r2950, 6;
	and.b32  	%r2952, %r2951, 76;
	add.s32 	%r2953, %r2952, %r2913;
	shl.b32 	%r2954, %r2953, 2;
	add.s32 	%r2955, %r479, %r2954;
	ld.shared.v4.u32 	{%r2723, %r2724, %r2725, %r2726}, [%r2955+16384];
	or.b32  	%r2956, %r2913, 5120;
	shr.u32 	%r2957, %r2956, 6;
	and.b32  	%r2958, %r2957, 92;
	add.s32 	%r2959, %r2958, %r2913;
	shl.b32 	%r2960, %r2959, 2;
	add.s32 	%r2961, %r479, %r2960;
	ld.shared.v4.u32 	{%r2727, %r2728, %r2729, %r2730}, [%r2961+20480];
	or.b32  	%r2962, %r2913, 6144;
	shr.u32 	%r2963, %r2962, 6;
	and.b32  	%r2964, %r2963, 108;
	add.s32 	%r2965, %r2964, %r2913;
	shl.b32 	%r2966, %r2965, 2;
	add.s32 	%r2967, %r479, %r2966;
	ld.shared.v4.u32 	{%r2731, %r2732, %r2733, %r2734}, [%r2967+24576];
	or.b32  	%r2968, %r2913, 7168;
	shr.u32 	%r2969, %r2968, 6;
	and.b32  	%r2970, %r2969, 124;
	add.s32 	%r2971, %r2970, %r2913;
	shl.b32 	%r2972, %r2971, 2;
	add.s32 	%r2973, %r479, %r2972;
	ld.shared.v4.u32 	{%r2735, %r2736, %r2737, %r2738}, [%r2973+28672];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2515 + 0 ], { %r3008, %r3009 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2518 + 0 ], { %r3010, %r3011 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2521 + 0 ], { %r3012, %r3013 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2524 + 0 ], { %r3014, %r3015 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2527 + 0 ], { %r3016, %r3017 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2530 + 0 ], { %r3018, %r3019 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2533 + 0 ], { %r3020, %r3021 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2536 + 0 ], { %r3022, %r3023 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2539 + 0 ], { %r3024, %r3025 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2542 + 0 ], { %r3026, %r3027 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2545 + 0 ], { %r3028, %r3029 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2548 + 0 ], { %r3030, %r3031 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2551 + 0 ], { %r3032, %r3033 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2554 + 0 ], { %r3034, %r3035 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2557 + 0 ], { %r3036, %r3037 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2560 + 0 ], { %r3038, %r3039 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2739, %r2740, %r2741, %r2742}, [%r2931];
	ld.shared.v4.u32 	{%r2743, %r2744, %r2745, %r2746}, [%r2937+4096];
	ld.shared.v4.u32 	{%r2747, %r2748, %r2749, %r2750}, [%r2943+8192];
	ld.shared.v4.u32 	{%r2751, %r2752, %r2753, %r2754}, [%r2949+12288];
	ld.shared.v4.u32 	{%r2755, %r2756, %r2757, %r2758}, [%r2955+16384];
	ld.shared.v4.u32 	{%r2759, %r2760, %r2761, %r2762}, [%r2961+20480];
	ld.shared.v4.u32 	{%r2763, %r2764, %r2765, %r2766}, [%r2967+24576];
	ld.shared.v4.u32 	{%r2767, %r2768, %r2769, %r2770}, [%r2973+28672];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2515 + 0 ], { %r3040, %r3041 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2518 + 0 ], { %r3042, %r3043 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2521 + 0 ], { %r3044, %r3045 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2524 + 0 ], { %r3046, %r3047 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2527 + 0 ], { %r3048, %r3049 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2530 + 0 ], { %r3050, %r3051 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2533 + 0 ], { %r3052, %r3053 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2536 + 0 ], { %r3054, %r3055 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2539 + 0 ], { %r3056, %r3057 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2542 + 0 ], { %r3058, %r3059 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2545 + 0 ], { %r3060, %r3061 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2548 + 0 ], { %r3062, %r3063 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2551 + 0 ], { %r3064, %r3065 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2554 + 0 ], { %r3066, %r3067 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2557 + 0 ], { %r3068, %r3069 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2560 + 0 ], { %r3070, %r3071 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2771, %r2772, %r2773, %r2774}, [%r2931];
	ld.shared.v4.u32 	{%r2775, %r2776, %r2777, %r2778}, [%r2937+4096];
	ld.shared.v4.u32 	{%r2779, %r2780, %r2781, %r2782}, [%r2943+8192];
	ld.shared.v4.u32 	{%r2783, %r2784, %r2785, %r2786}, [%r2949+12288];
	ld.shared.v4.u32 	{%r2787, %r2788, %r2789, %r2790}, [%r2955+16384];
	ld.shared.v4.u32 	{%r2791, %r2792, %r2793, %r2794}, [%r2961+20480];
	ld.shared.v4.u32 	{%r2795, %r2796, %r2797, %r2798}, [%r2967+24576];
	ld.shared.v4.u32 	{%r2799, %r2800, %r2801, %r2802}, [%r2973+28672];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2515 + 0 ], { %r3072, %r3073 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2518 + 0 ], { %r3074, %r3075 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2521 + 0 ], { %r3076, %r3077 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2524 + 0 ], { %r3078, %r3079 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2527 + 0 ], { %r3080, %r3081 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2530 + 0 ], { %r3082, %r3083 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2533 + 0 ], { %r3084, %r3085 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2536 + 0 ], { %r3086, %r3087 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2539 + 0 ], { %r3088, %r3089 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2542 + 0 ], { %r3090, %r3091 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2545 + 0 ], { %r3092, %r3093 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2548 + 0 ], { %r3094, %r3095 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2551 + 0 ], { %r3096, %r3097 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2554 + 0 ], { %r3098, %r3099 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2557 + 0 ], { %r3100, %r3101 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b32 [ %r2560 + 0 ], { %r3102, %r3103 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r2803, %r2804, %r2805, %r2806}, [%r2931];
	ld.shared.v4.u32 	{%r2807, %r2808, %r2809, %r2810}, [%r2937+4096];
	ld.shared.v4.u32 	{%r2811, %r2812, %r2813, %r2814}, [%r2943+8192];
	ld.shared.v4.u32 	{%r2815, %r2816, %r2817, %r2818}, [%r2949+12288];
	ld.shared.v4.u32 	{%r2819, %r2820, %r2821, %r2822}, [%r2955+16384];
	ld.shared.v4.u32 	{%r2823, %r2824, %r2825, %r2826}, [%r2961+20480];
	ld.shared.v4.u32 	{%r2827, %r2828, %r2829, %r2830}, [%r2967+24576];
	ld.shared.v4.u32 	{%r2831, %r2832, %r2833, %r2834}, [%r2973+28672];
	// begin inline asm
	@%p69 st.global.v4.b32 [ %rd106 + 0 ], { %r2707, %r2708, %r2709, %r2710 };
	// end inline asm
	// begin inline asm
	@%p70 st.global.v4.b32 [ %rd107 + 0 ], { %r2711, %r2712, %r2713, %r2714 };
	// end inline asm
	// begin inline asm
	@%p71 st.global.v4.b32 [ %rd108 + 0 ], { %r2715, %r2716, %r2717, %r2718 };
	// end inline asm
	// begin inline asm
	@%p72 st.global.v4.b32 [ %rd109 + 0 ], { %r2719, %r2720, %r2721, %r2722 };
	// end inline asm
	// begin inline asm
	@%p73 st.global.v4.b32 [ %rd110 + 0 ], { %r2723, %r2724, %r2725, %r2726 };
	// end inline asm
	// begin inline asm
	@%p74 st.global.v4.b32 [ %rd111 + 0 ], { %r2727, %r2728, %r2729, %r2730 };
	// end inline asm
	// begin inline asm
	@%p75 st.global.v4.b32 [ %rd112 + 0 ], { %r2731, %r2732, %r2733, %r2734 };
	// end inline asm
	// begin inline asm
	@%p76 st.global.v4.b32 [ %rd113 + 0 ], { %r2735, %r2736, %r2737, %r2738 };
	// end inline asm
	// begin inline asm
	@%p77 st.global.v4.b32 [ %rd114 + 0 ], { %r2739, %r2740, %r2741, %r2742 };
	// end inline asm
	// begin inline asm
	@%p78 st.global.v4.b32 [ %rd115 + 0 ], { %r2743, %r2744, %r2745, %r2746 };
	// end inline asm
	// begin inline asm
	@%p79 st.global.v4.b32 [ %rd116 + 0 ], { %r2747, %r2748, %r2749, %r2750 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd117 + 0 ], { %r2751, %r2752, %r2753, %r2754 };
	// end inline asm
	// begin inline asm
	@%p81 st.global.v4.b32 [ %rd118 + 0 ], { %r2755, %r2756, %r2757, %r2758 };
	// end inline asm
	// begin inline asm
	@%p82 st.global.v4.b32 [ %rd119 + 0 ], { %r2759, %r2760, %r2761, %r2762 };
	// end inline asm
	// begin inline asm
	@%p83 st.global.v4.b32 [ %rd120 + 0 ], { %r2763, %r2764, %r2765, %r2766 };
	// end inline asm
	// begin inline asm
	@%p84 st.global.v4.b32 [ %rd121 + 0 ], { %r2767, %r2768, %r2769, %r2770 };
	// end inline asm
	// begin inline asm
	@%p85 st.global.v4.b32 [ %rd122 + 0 ], { %r2771, %r2772, %r2773, %r2774 };
	// end inline asm
	// begin inline asm
	@%p86 st.global.v4.b32 [ %rd123 + 0 ], { %r2775, %r2776, %r2777, %r2778 };
	// end inline asm
	// begin inline asm
	@%p87 st.global.v4.b32 [ %rd124 + 0 ], { %r2779, %r2780, %r2781, %r2782 };
	// end inline asm
	// begin inline asm
	@%p88 st.global.v4.b32 [ %rd125 + 0 ], { %r2783, %r2784, %r2785, %r2786 };
	// end inline asm
	// begin inline asm
	@%p89 st.global.v4.b32 [ %rd126 + 0 ], { %r2787, %r2788, %r2789, %r2790 };
	// end inline asm
	// begin inline asm
	@%p90 st.global.v4.b32 [ %rd127 + 0 ], { %r2791, %r2792, %r2793, %r2794 };
	// end inline asm
	// begin inline asm
	@%p91 st.global.v4.b32 [ %rd128 + 0 ], { %r2795, %r2796, %r2797, %r2798 };
	// end inline asm
	// begin inline asm
	@%p92 st.global.v4.b32 [ %rd129 + 0 ], { %r2799, %r2800, %r2801, %r2802 };
	// end inline asm
	// begin inline asm
	@%p93 st.global.v4.b32 [ %rd130 + 0 ], { %r2803, %r2804, %r2805, %r2806 };
	// end inline asm
	// begin inline asm
	@%p94 st.global.v4.b32 [ %rd131 + 0 ], { %r2807, %r2808, %r2809, %r2810 };
	// end inline asm
	// begin inline asm
	@%p95 st.global.v4.b32 [ %rd132 + 0 ], { %r2811, %r2812, %r2813, %r2814 };
	// end inline asm
	// begin inline asm
	@%p96 st.global.v4.b32 [ %rd133 + 0 ], { %r2815, %r2816, %r2817, %r2818 };
	// end inline asm
	// begin inline asm
	@%p97 st.global.v4.b32 [ %rd134 + 0 ], { %r2819, %r2820, %r2821, %r2822 };
	// end inline asm
	// begin inline asm
	@%p98 st.global.v4.b32 [ %rd135 + 0 ], { %r2823, %r2824, %r2825, %r2826 };
	// end inline asm
	// begin inline asm
	@%p99 st.global.v4.b32 [ %rd136 + 0 ], { %r2827, %r2828, %r2829, %r2830 };
	// end inline asm
	// begin inline asm
	@%p100 st.global.v4.b32 [ %rd137 + 0 ], { %r2831, %r2832, %r2833, %r2834 };
	// end inline asm
	.loc	1 89 4                          // cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py:89:4
	ret;
$L__tmp1:
$L__func_end0:
                                        // -- End function
}
	.file	1 "/tmp/torchinductor_root/uh/cuhb4zeb4cjhalvetob5fmvjbfbugxtpivvucb4h556ebwgppvd6.py"
	.section	.debug_abbrev
	{
.b8 1                                   // Abbreviation Code
.b8 17                                  // DW_TAG_compile_unit
.b8 0                                   // DW_CHILDREN_no
.b8 37                                  // DW_AT_producer
.b8 8                                   // DW_FORM_string
.b8 19                                  // DW_AT_language
.b8 5                                   // DW_FORM_data2
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 16                                  // DW_AT_stmt_list
.b8 6                                   // DW_FORM_data4
.b8 27                                  // DW_AT_comp_dir
.b8 8                                   // DW_FORM_string
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 0                                   // EOM(3)
	}
	.section	.debug_info
	{
.b32 104                                // Length of Unit
.b8 2                                   // DWARF version number
.b8 0
.b32 .debug_abbrev                      // Offset Into Abbrev. Section
.b8 8                                   // Address Size (in bytes)
.b8 1                                   // Abbrev [1] 0xb:0x61 DW_TAG_compile_unit
.b8 116                                 // DW_AT_producer
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2                                   // DW_AT_language
.b8 0
.b8 99                                  // DW_AT_name
.b8 117
.b8 104
.b8 98
.b8 52
.b8 122
.b8 101
.b8 98
.b8 52
.b8 99
.b8 106
.b8 104
.b8 97
.b8 108
.b8 118
.b8 101
.b8 116
.b8 111
.b8 98
.b8 53
.b8 102
.b8 109
.b8 118
.b8 106
.b8 98
.b8 102
.b8 98
.b8 117
.b8 103
.b8 120
.b8 116
.b8 112
.b8 105
.b8 118
.b8 118
.b8 117
.b8 99
.b8 98
.b8 52
.b8 104
.b8 53
.b8 53
.b8 54
.b8 101
.b8 98
.b8 119
.b8 103
.b8 112
.b8 112
.b8 118
.b8 100
.b8 54
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line                        // DW_AT_stmt_list
.b8 47                                  // DW_AT_comp_dir
.b8 116
.b8 109
.b8 112
.b8 47
.b8 116
.b8 111
.b8 114
.b8 99
.b8 104
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 114
.b8 111
.b8 111
.b8 116
.b8 47
.b8 117
.b8 104
.b8 0
	}
	.section	.debug_macinfo	{	}
