//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_86
.address_size 64

	// .globl	triton_mm               // -- Begin function triton_mm
.extern .shared .align 16 .b8 global_smem[];
                                        // @triton_mm
.visible .entry triton_mm(
	.param .u64 .ptr .global .align 1 triton_mm_param_0,
	.param .u64 .ptr .global .align 1 triton_mm_param_1,
	.param .u64 .ptr .global .align 1 triton_mm_param_2,
	.param .u64 .ptr .global .align 1 triton_mm_param_3
)
.reqntid 128, 1, 1
{
	.reg .pred 	%p<54>;
	.reg .b16 	%rs<65>;
	.reg .b32 	%r<930>;
	.reg .f32 	%f<706>;
	.reg .b64 	%rd<173>;
	.loc	1 17 0                          // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:17:0
$L__func_begin0:
	.loc	1 17 0                          // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:17:0

// %bb.0:
	ld.param.u64 	%rd17, [triton_mm_param_2];
	ld.param.u64 	%rd43, [triton_mm_param_0];
	ld.param.u64 	%rd44, [triton_mm_param_1];
$L__tmp0:
	.loc	1 40 24                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:40:24
	mov.u32 	%r85, %ctaid.x;
	.loc	1 46 22                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:46:22
	mul.hi.s32 	%r86, %r85, 715827883;
	shr.u32 	%r87, %r86, 31;
	shr.s32 	%r88, %r86, 3;
	add.s32 	%r89, %r88, %r87;
	.loc	1 47 41                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:47:41
	shl.b32 	%r90, %r89, 3;
	.loc	1 47 30                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:47:30
	sub.s32 	%r91, 2, %r90;
	.loc	1 47 50                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:47:50
	min.s32 	%r92, %r91, 8;
	.loc	1 48 40                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:48:40
	rem.s32 	%r93, %r85, %r92;
	.loc	1 48 34                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:48:34
	add.s32 	%r94, %r93, %r90;
	.loc	1 49 19                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:49:19
	mul.lo.s32 	%r95, %r89, 48;
	sub.s32 	%r96, %r85, %r95;
	.loc	1 49 30                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:49:30
	div.s32 	%r97, %r96, %r92;
	.loc	1 51 17                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:51:17
	shl.b32 	%r1, %r94, 6;
	.loc	1 51 40                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:51:40
	mov.u32 	%r2, %tid.x;
	and.b32  	%r3, %r2, 8;
	and.b32  	%r4, %r2, 16;
	and.b32  	%r98, %r2, 32;
	bfe.u32 	%r99, %r2, 3, 4;
	or.b32  	%r100, %r99, 16;
	or.b32  	%r101, %r99, 32;
	or.b32  	%r102, %r99, 48;
	.loc	1 51 27                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:51:27
	or.b32  	%r103, %r1, %r99;
	or.b32  	%r104, %r1, %r100;
	or.b32  	%r105, %r1, %r101;
	or.b32  	%r106, %r1, %r102;
	.loc	1 52 17                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:52:17
	shl.b32 	%r5, %r97, 7;
	.loc	1 52 40                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:52:40
	shl.b32 	%r6, %r2, 3;
	and.b32  	%r107, %r6, 8;
	and.b32  	%r108, %r6, 16;
	and.b32  	%r109, %r6, 24;
	and.b32  	%r110, %r6, 32;
	and.b32  	%r7, %r6, 56;
	.loc	1 52 27                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:52:27
	or.b32  	%r111, %r5, %r99;
	or.b32  	%r112, %r5, %r100;
	or.b32  	%r113, %r5, %r101;
	or.b32  	%r114, %r5, %r102;
	or.b32  	%r115, %r111, 64;
	or.b32  	%r116, %r111, 80;
	or.b32  	%r117, %r111, 96;
	or.b32  	%r118, %r111, 112;
	.loc	1 54 57                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:54:57
	mul.hi.s32 	%r119, %r103, 892460737;
	shr.u32 	%r120, %r119, 31;
	shr.u32 	%r121, %r119, 4;
	add.s32 	%r122, %r121, %r120;
	mul.lo.s32 	%r123, %r122, 77;
	sub.s32 	%r124, %r103, %r123;
	mul.hi.s32 	%r125, %r104, 892460737;
	shr.u32 	%r126, %r125, 31;
	shr.u32 	%r127, %r125, 4;
	add.s32 	%r128, %r127, %r126;
	mul.lo.s32 	%r129, %r128, 77;
	sub.s32 	%r130, %r104, %r129;
	mul.hi.s32 	%r131, %r105, 892460737;
	shr.u32 	%r132, %r131, 31;
	shr.u32 	%r133, %r131, 4;
	add.s32 	%r134, %r133, %r132;
	mul.lo.s32 	%r135, %r134, 77;
	sub.s32 	%r136, %r105, %r135;
	mul.hi.s32 	%r137, %r106, 892460737;
	shr.u32 	%r138, %r137, 31;
	shr.u32 	%r139, %r137, 4;
	add.s32 	%r140, %r139, %r138;
	mul.lo.s32 	%r141, %r140, 77;
	sub.s32 	%r142, %r106, %r141;
	.loc	1 58 57                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:58:57
	mul.hi.s32 	%r143, %r111, 715827883;
	shr.u32 	%r144, %r143, 31;
	shr.u32 	%r145, %r143, 7;
	add.s32 	%r146, %r145, %r144;
	mul.lo.s32 	%r147, %r146, 768;
	sub.s32 	%r148, %r111, %r147;
	mul.hi.s32 	%r149, %r112, 715827883;
	shr.u32 	%r150, %r149, 31;
	shr.u32 	%r151, %r149, 7;
	add.s32 	%r152, %r151, %r150;
	mul.lo.s32 	%r153, %r152, 768;
	sub.s32 	%r154, %r112, %r153;
	mul.hi.s32 	%r155, %r113, 715827883;
	shr.u32 	%r156, %r155, 31;
	shr.u32 	%r157, %r155, 7;
	add.s32 	%r158, %r157, %r156;
	mul.lo.s32 	%r159, %r158, 768;
	sub.s32 	%r160, %r113, %r159;
	mul.hi.s32 	%r161, %r114, 715827883;
	shr.u32 	%r162, %r161, 31;
	shr.u32 	%r163, %r161, 7;
	add.s32 	%r164, %r163, %r162;
	mul.lo.s32 	%r165, %r164, 768;
	sub.s32 	%r166, %r114, %r165;
	mul.hi.s32 	%r167, %r115, 715827883;
	shr.u32 	%r168, %r167, 31;
	shr.u32 	%r169, %r167, 7;
	add.s32 	%r170, %r169, %r168;
	mul.lo.s32 	%r171, %r170, 768;
	sub.s32 	%r172, %r115, %r171;
	mul.hi.s32 	%r173, %r116, 715827883;
	shr.u32 	%r174, %r173, 31;
	shr.u32 	%r175, %r173, 7;
	add.s32 	%r176, %r175, %r174;
	mul.lo.s32 	%r177, %r176, 768;
	sub.s32 	%r178, %r116, %r177;
	mul.hi.s32 	%r179, %r117, 715827883;
	shr.u32 	%r180, %r179, 31;
	shr.u32 	%r181, %r179, 7;
	add.s32 	%r182, %r181, %r180;
	mul.lo.s32 	%r183, %r182, 768;
	sub.s32 	%r184, %r117, %r183;
	mul.hi.s32 	%r185, %r118, 715827883;
	shr.u32 	%r186, %r185, 31;
	shr.u32 	%r187, %r185, 7;
	add.s32 	%r188, %r187, %r186;
	mul.lo.s32 	%r189, %r188, 768;
	sub.s32 	%r190, %r118, %r189;
	.loc	1 71 29                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:71:29
	mul.lo.s32 	%r191, %r124, 768;
	mul.lo.s32 	%r192, %r130, 768;
	mul.lo.s32 	%r193, %r136, 768;
	mul.lo.s32 	%r194, %r142, 768;
	.loc	1 77 54                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:54
	mul.lo.s32 	%r195, %r148, 768;
	mul.lo.s32 	%r196, %r154, 768;
	mul.lo.s32 	%r197, %r160, 768;
	mul.lo.s32 	%r198, %r166, 768;
	mul.lo.s32 	%r199, %r172, 768;
	mul.lo.s32 	%r200, %r178, 768;
	mul.lo.s32 	%r201, %r184, 768;
	mul.lo.s32 	%r202, %r190, 768;
	.loc	1 71 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:71:25
	or.b32  	%r203, %r191, %r7;
	or.b32  	%r204, %r192, %r7;
	or.b32  	%r205, %r193, %r7;
	or.b32  	%r206, %r194, %r7;
	.loc	1 72 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:25
	mul.wide.s32 	%rd45, %r203, 2;
	add.s64 	%rd18, %rd43, %rd45;
	mul.wide.s32 	%rd46, %r204, 2;
	add.s64 	%rd19, %rd43, %rd46;
	mul.wide.s32 	%rd47, %r205, 2;
	add.s64 	%rd20, %rd43, %rd47;
	mul.wide.s32 	%rd48, %r206, 2;
	add.s64 	%rd21, %rd43, %rd48;
	.loc	1 72 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:20
	and.b32  	%r207, %r2, 24;
	xor.b32  	%r8, %r7, %r207;
	xor.b32  	%r208, %r8, %r98;
	shl.b32 	%r209, %r99, 6;
	or.b32  	%r9, %r208, %r209;
	shl.b32 	%r210, %r9, 1;
	mov.u32 	%r211, global_smem;
	add.s32 	%r43, %r211, %r210;
	add.s32 	%r35, %r43, 32768;
	add.s32 	%r37, %r43, 34816;
	add.s32 	%r39, %r43, 36864;
	add.s32 	%r41, %r43, 38912;
	mov.b32 	%r36, 16;
	// begin inline asm
	cp.async.cg.shared.global [ %r35 + 0 ], [ %rd18 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r37 + 0 ], [ %rd19 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r39 + 0 ], [ %rd20 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r41 + 0 ], [ %rd21 + 0 ], 0x10, %r36;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 50                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:50
	or.b32  	%r212, %r195, %r7;
	or.b32  	%r213, %r196, %r7;
	or.b32  	%r214, %r197, %r7;
	or.b32  	%r215, %r198, %r7;
	or.b32  	%r216, %r199, %r7;
	or.b32  	%r217, %r200, %r7;
	or.b32  	%r218, %r201, %r7;
	or.b32  	%r219, %r202, %r7;
	.loc	1 77 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:25
	mul.wide.s32 	%rd49, %r212, 2;
	add.s64 	%rd22, %rd44, %rd49;
	mul.wide.s32 	%rd50, %r213, 2;
	add.s64 	%rd23, %rd44, %rd50;
	mul.wide.s32 	%rd51, %r214, 2;
	add.s64 	%rd24, %rd44, %rd51;
	mul.wide.s32 	%rd52, %r215, 2;
	add.s64 	%rd25, %rd44, %rd52;
	mul.wide.s32 	%rd53, %r216, 2;
	add.s64 	%rd26, %rd44, %rd53;
	mul.wide.s32 	%rd54, %r217, 2;
	add.s64 	%rd27, %rd44, %rd54;
	mul.wide.s32 	%rd55, %r218, 2;
	add.s64 	%rd28, %rd44, %rd55;
	mul.wide.s32 	%rd56, %r219, 2;
	add.s64 	%rd29, %rd44, %rd56;
	.loc	1 77 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:20
	add.s32 	%r45, %r43, 2048;
	add.s32 	%r47, %r43, 4096;
	add.s32 	%r49, %r43, 6144;
	add.s32 	%r51, %r43, 8192;
	add.s32 	%r53, %r43, 10240;
	add.s32 	%r55, %r43, 12288;
	add.s32 	%r57, %r43, 14336;
	// begin inline asm
	cp.async.cg.shared.global [ %r43 + 0 ], [ %rd22 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r45 + 0 ], [ %rd23 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r47 + 0 ], [ %rd24 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r49 + 0 ], [ %rd25 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r51 + 0 ], [ %rd26 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r53 + 0 ], [ %rd27 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r55 + 0 ], [ %rd28 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r57 + 0 ], [ %rd29 + 0 ], 0x10, %r36;
	// end inline asm
	cp.async.commit_group;
	.loc	1 72 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:25
	cvt.s64.s32 	%rd57, %r191;
	cvt.u64.u32 	%rd58, %r7;
	or.b64  	%rd59, %rd57, %rd58;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd43, %rd60;
	add.s64 	%rd30, %rd61, 128;
	cvt.s64.s32 	%rd62, %r192;
	or.b64  	%rd63, %rd62, %rd58;
	shl.b64 	%rd64, %rd63, 1;
	add.s64 	%rd65, %rd43, %rd64;
	add.s64 	%rd31, %rd65, 128;
	cvt.s64.s32 	%rd66, %r193;
	or.b64  	%rd67, %rd66, %rd58;
	shl.b64 	%rd68, %rd67, 1;
	add.s64 	%rd69, %rd43, %rd68;
	add.s64 	%rd32, %rd69, 128;
	cvt.s64.s32 	%rd70, %r194;
	or.b64  	%rd71, %rd70, %rd58;
	shl.b64 	%rd72, %rd71, 1;
	add.s64 	%rd73, %rd43, %rd72;
	add.s64 	%rd33, %rd73, 128;
	.loc	1 72 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:20
	bar.sync 	0;
	add.s32 	%r59, %r43, 40960;
	add.s32 	%r61, %r43, 43008;
	add.s32 	%r63, %r43, 45056;
	add.s32 	%r65, %r43, 47104;
	// begin inline asm
	cp.async.cg.shared.global [ %r59 + 0 ], [ %rd30 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r61 + 0 ], [ %rd31 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r63 + 0 ], [ %rd32 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r65 + 0 ], [ %rd33 + 0 ], 0x10, %r36;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:25
	cvt.s64.s32 	%rd74, %r195;
	or.b64  	%rd75, %rd74, %rd58;
	shl.b64 	%rd76, %rd75, 1;
	add.s64 	%rd77, %rd44, %rd76;
	add.s64 	%rd34, %rd77, 128;
	cvt.s64.s32 	%rd78, %r196;
	or.b64  	%rd79, %rd78, %rd58;
	shl.b64 	%rd80, %rd79, 1;
	add.s64 	%rd81, %rd44, %rd80;
	add.s64 	%rd35, %rd81, 128;
	cvt.s64.s32 	%rd82, %r197;
	or.b64  	%rd83, %rd82, %rd58;
	shl.b64 	%rd84, %rd83, 1;
	add.s64 	%rd85, %rd44, %rd84;
	add.s64 	%rd36, %rd85, 128;
	cvt.s64.s32 	%rd86, %r198;
	or.b64  	%rd87, %rd86, %rd58;
	shl.b64 	%rd88, %rd87, 1;
	add.s64 	%rd89, %rd44, %rd88;
	add.s64 	%rd37, %rd89, 128;
	cvt.s64.s32 	%rd90, %r199;
	or.b64  	%rd91, %rd90, %rd58;
	shl.b64 	%rd92, %rd91, 1;
	add.s64 	%rd93, %rd44, %rd92;
	add.s64 	%rd38, %rd93, 128;
	cvt.s64.s32 	%rd94, %r200;
	or.b64  	%rd95, %rd94, %rd58;
	shl.b64 	%rd96, %rd95, 1;
	add.s64 	%rd97, %rd44, %rd96;
	add.s64 	%rd39, %rd97, 128;
	cvt.s64.s32 	%rd98, %r201;
	or.b64  	%rd99, %rd98, %rd58;
	shl.b64 	%rd100, %rd99, 1;
	add.s64 	%rd101, %rd44, %rd100;
	add.s64 	%rd40, %rd101, 128;
	cvt.s64.s32 	%rd102, %r202;
	or.b64  	%rd103, %rd102, %rd58;
	shl.b64 	%rd104, %rd103, 1;
	add.s64 	%rd105, %rd44, %rd104;
	add.s64 	%rd41, %rd105, 128;
	.loc	1 77 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:20
	add.s32 	%r67, %r43, 16384;
	add.s32 	%r69, %r43, 18432;
	add.s32 	%r71, %r43, 20480;
	add.s32 	%r73, %r43, 22528;
	add.s32 	%r75, %r43, 24576;
	add.s32 	%r77, %r43, 26624;
	add.s32 	%r79, %r43, 28672;
	add.s32 	%r81, %r43, 30720;
	// begin inline asm
	cp.async.cg.shared.global [ %r67 + 0 ], [ %rd34 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r69 + 0 ], [ %rd35 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r71 + 0 ], [ %rd36 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r73 + 0 ], [ %rd37 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r75 + 0 ], [ %rd38 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r77 + 0 ], [ %rd39 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r79 + 0 ], [ %rd40 + 0 ], 0x10, %r36;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r81 + 0 ], [ %rd41 + 0 ], 0x10, %r36;
	// end inline asm
	cp.async.commit_group;
	and.b32  	%r220, %r2, 7;
	.loc	1 64 26                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:64:26
	or.b32  	%r221, %r3, %r220;
	shr.u32 	%r222, %r4, 1;
	xor.b32  	%r10, %r7, %r222;
	shl.b32 	%r223, %r2, 6;
	and.b32  	%r11, %r223, 960;
	or.b32  	%r12, %r10, %r11;
	or.b32  	%r224, %r107, 16;
	xor.b32  	%r225, %r224, %r108;
	or.b32  	%r226, %r225, %r110;
	xor.b32  	%r13, %r226, %r222;
	or.b32  	%r14, %r13, %r11;
	or.b32  	%r227, %r109, 32;
	xor.b32  	%r228, %r227, %r110;
	xor.b32  	%r15, %r228, %r222;
	or.b32  	%r16, %r15, %r11;
	or.b32  	%r229, %r107, 48;
	and.b32  	%r230, %r6, 48;
	or.b32  	%r231, %r222, %r230;
	xor.b32  	%r17, %r231, %r229;
	or.b32  	%r18, %r17, %r11;
	shr.u32 	%r232, %r98, 2;
	or.b32  	%r233, %r220, %r232;
	shl.b32 	%r234, %r221, 6;
	or.b32  	%r235, %r234, 2048;
	or.b32  	%r19, %r235, %r10;
	or.b32  	%r20, %r13, %r235;
	or.b32  	%r21, %r15, %r235;
	or.b32  	%r22, %r17, %r235;
	shr.u32 	%r23, %r2, 2;
	and.b32  	%r236, %r23, 16;
	or.b32  	%r237, %r236, %r232;
	or.b32  	%r238, %r237, %r220;
	shl.b32 	%r24, %r238, 6;
	or.b32  	%r25, %r24, %r8;
	xor.b32  	%r239, %r228, %r207;
	or.b32  	%r26, %r239, %r24;
	or.b32  	%r240, %r233, %r236;
	shl.b32 	%r241, %r240, 6;
	or.b32  	%r242, %r241, 2048;
	or.b32  	%r27, %r242, %r8;
	or.b32  	%r28, %r239, %r242;
	or.b32  	%r29, %r26, 4096;
	or.b32  	%r30, %r26, 6144;
	mul.wide.u32 	%rd106, %r220, 16;
	mul.wide.s32 	%rd107, %r202, 2;
	or.b64  	%rd108, %rd106, %rd107;
	add.s64 	%rd109, %rd108, %rd44;
	add.s64 	%rd1, %rd109, 256;
	mul.wide.s32 	%rd110, %r201, 2;
	or.b64  	%rd111, %rd106, %rd110;
	add.s64 	%rd112, %rd111, %rd44;
	add.s64 	%rd2, %rd112, 256;
	mul.wide.s32 	%rd113, %r200, 2;
	or.b64  	%rd114, %rd106, %rd113;
	add.s64 	%rd115, %rd114, %rd44;
	add.s64 	%rd3, %rd115, 256;
	mul.wide.s32 	%rd116, %r199, 2;
	or.b64  	%rd117, %rd106, %rd116;
	add.s64 	%rd118, %rd117, %rd44;
	add.s64 	%rd4, %rd118, 256;
	mul.wide.s32 	%rd119, %r198, 2;
	or.b64  	%rd120, %rd106, %rd119;
	add.s64 	%rd121, %rd120, %rd44;
	add.s64 	%rd5, %rd121, 256;
	mul.wide.s32 	%rd122, %r197, 2;
	or.b64  	%rd123, %rd106, %rd122;
	add.s64 	%rd124, %rd123, %rd44;
	add.s64 	%rd6, %rd124, 256;
	mul.wide.s32 	%rd125, %r196, 2;
	or.b64  	%rd126, %rd106, %rd125;
	add.s64 	%rd127, %rd126, %rd44;
	add.s64 	%rd7, %rd127, 256;
	mul.wide.s32 	%rd128, %r195, 2;
	or.b64  	%rd129, %rd106, %rd128;
	add.s64 	%rd130, %rd129, %rd44;
	add.s64 	%rd8, %rd130, 256;
	mul.wide.s32 	%rd131, %r194, 2;
	or.b64  	%rd132, %rd106, %rd131;
	add.s64 	%rd133, %rd132, %rd43;
	add.s64 	%rd9, %rd133, 256;
	mul.wide.s32 	%rd134, %r193, 2;
	or.b64  	%rd135, %rd106, %rd134;
	add.s64 	%rd136, %rd135, %rd43;
	add.s64 	%rd10, %rd136, 256;
	mul.wide.s32 	%rd137, %r192, 2;
	or.b64  	%rd138, %rd106, %rd137;
	add.s64 	%rd139, %rd138, %rd43;
	add.s64 	%rd11, %rd139, 256;
	mul.wide.s32 	%rd140, %r191, 2;
	or.b64  	%rd141, %rd106, %rd140;
	add.s64 	%rd142, %rd141, %rd43;
	add.s64 	%rd12, %rd142, 256;
	mov.f32 	%f642, 0f00000000;
	mov.b32 	%r929, 1;
	mov.b32 	%r928, -1;
	mov.b64 	%rd171, 0;
	shl.b32 	%r776, %r12, 1;
	shl.b32 	%r777, %r14, 1;
	shl.b32 	%r778, %r16, 1;
	shl.b32 	%r779, %r18, 1;
	shl.b32 	%r792, %r19, 1;
	shl.b32 	%r793, %r20, 1;
	shl.b32 	%r794, %r21, 1;
	shl.b32 	%r795, %r22, 1;
	shl.b32 	%r798, %r25, 1;
	shl.b32 	%r800, %r27, 1;
	shl.b32 	%r801, %r28, 1;
	shl.b32 	%r805, %r29, 1;
	shl.b32 	%r806, %r30, 1;
	mov.u64 	%rd172, %rd171;
	mov.f32 	%f643, %f642;
	mov.f32 	%f644, %f642;
	mov.f32 	%f645, %f642;
	mov.f32 	%f646, %f642;
	mov.f32 	%f647, %f642;
	mov.f32 	%f648, %f642;
	mov.f32 	%f649, %f642;
	mov.f32 	%f650, %f642;
	mov.f32 	%f651, %f642;
	mov.f32 	%f652, %f642;
	mov.f32 	%f653, %f642;
	mov.f32 	%f654, %f642;
	mov.f32 	%f655, %f642;
	mov.f32 	%f656, %f642;
	mov.f32 	%f657, %f642;
	mov.f32 	%f658, %f642;
	mov.f32 	%f659, %f642;
	mov.f32 	%f660, %f642;
	mov.f32 	%f661, %f642;
	mov.f32 	%f662, %f642;
	mov.f32 	%f663, %f642;
	mov.f32 	%f664, %f642;
	mov.f32 	%f665, %f642;
	mov.f32 	%f666, %f642;
	mov.f32 	%f667, %f642;
	mov.f32 	%f668, %f642;
	mov.f32 	%f669, %f642;
	mov.f32 	%f670, %f642;
	mov.f32 	%f671, %f642;
	mov.f32 	%f672, %f642;
	mov.f32 	%f673, %f642;
	mov.f32 	%f674, %f642;
	mov.f32 	%f675, %f642;
	mov.f32 	%f676, %f642;
	mov.f32 	%f677, %f642;
	mov.f32 	%f678, %f642;
	mov.f32 	%f679, %f642;
	mov.f32 	%f680, %f642;
	mov.f32 	%f681, %f642;
	mov.f32 	%f682, %f642;
	mov.f32 	%f683, %f642;
	mov.f32 	%f684, %f642;
	mov.f32 	%f685, %f642;
	mov.f32 	%f686, %f642;
	mov.f32 	%f687, %f642;
	mov.f32 	%f688, %f642;
	mov.f32 	%f689, %f642;
	mov.f32 	%f690, %f642;
	mov.f32 	%f691, %f642;
	mov.f32 	%f692, %f642;
	mov.f32 	%f693, %f642;
	mov.f32 	%f694, %f642;
	mov.f32 	%f695, %f642;
	mov.f32 	%f696, %f642;
	mov.f32 	%f697, %f642;
	mov.f32 	%f698, %f642;
	mov.f32 	%f699, %f642;
	mov.f32 	%f700, %f642;
	mov.f32 	%f701, %f642;
	mov.f32 	%f702, %f642;
	mov.f32 	%f703, %f642;
	mov.f32 	%f704, %f642;
	mov.f32 	%f705, %f642;
$L__BB0_1:                              // =>This Inner Loop Header: Depth=1
	setp.lt.u64 	%p1, %rd172, 10;
	add.s32 	%r771, %r928, 1;
	setp.lt.s32 	%p2, %r771, 2;
	selp.b32 	%r928, %r771, 0, %p2;
	.loc	1 72 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:20
	cp.async.wait_group 2;
	bar.sync 	0;
	shl.b32 	%r772, %r928, 13;
	add.s32 	%r774, %r211, 32768;
	add.s32 	%r775, %r774, %r772;
	add.s32 	%r247, %r775, %r776;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r363, %r364, %r365, %r366}, [%r247];
	// end inline asm
	add.s32 	%r252, %r775, %r777;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r459, %r460, %r461, %r462}, [%r252];
	// end inline asm
	add.s32 	%r257, %r775, %r778;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r555, %r556, %r557, %r558}, [%r257];
	// end inline asm
	add.s32 	%r262, %r775, %r779;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r651, %r652, %r653, %r654}, [%r262];
	// end inline asm
	add.s32 	%r780, %r10, %r11;
	shl.b32 	%r781, %r780, 1;
	add.s32 	%r782, %r775, %r781;
	add.s32 	%r267, %r782, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r387, %r388, %r389, %r390}, [%r267];
	// end inline asm
	add.s32 	%r783, %r13, %r11;
	shl.b32 	%r784, %r783, 1;
	add.s32 	%r785, %r775, %r784;
	add.s32 	%r272, %r785, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r483, %r484, %r485, %r486}, [%r272];
	// end inline asm
	add.s32 	%r786, %r15, %r11;
	shl.b32 	%r787, %r786, 1;
	add.s32 	%r788, %r775, %r787;
	add.s32 	%r277, %r788, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r579, %r580, %r581, %r582}, [%r277];
	// end inline asm
	add.s32 	%r789, %r17, %r11;
	shl.b32 	%r790, %r789, 1;
	add.s32 	%r791, %r775, %r790;
	add.s32 	%r282, %r791, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r675, %r676, %r677, %r678}, [%r282];
	// end inline asm
	add.s32 	%r287, %r775, %r792;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r411, %r412, %r413, %r414}, [%r287];
	// end inline asm
	add.s32 	%r292, %r775, %r793;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r507, %r508, %r509, %r510}, [%r292];
	// end inline asm
	add.s32 	%r297, %r775, %r794;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r603, %r604, %r605, %r606}, [%r297];
	// end inline asm
	add.s32 	%r302, %r775, %r795;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r699, %r700, %r701, %r702}, [%r302];
	// end inline asm
	add.s32 	%r307, %r782, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r435, %r436, %r437, %r438}, [%r307];
	// end inline asm
	add.s32 	%r312, %r785, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r531, %r532, %r533, %r534}, [%r312];
	// end inline asm
	add.s32 	%r317, %r788, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r627, %r628, %r629, %r630}, [%r317];
	// end inline asm
	add.s32 	%r322, %r791, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r723, %r724, %r725, %r726}, [%r322];
	// end inline asm
	.loc	1 77 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:20
	shl.b32 	%r796, %r928, 14;
	add.s32 	%r797, %r211, %r796;
	add.s32 	%r327, %r797, %r798;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r367, %r368, %r463, %r464}, [%r327];
	// end inline asm
	shl.b32 	%r799, %r26, 1;
	add.s32 	%r332, %r797, %r799;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r559, %r560, %r655, %r656}, [%r332];
	// end inline asm
	add.s32 	%r337, %r797, %r800;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r373, %r374, %r469, %r470}, [%r337];
	// end inline asm
	add.s32 	%r342, %r797, %r801;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r565, %r566, %r661, %r662}, [%r342];
	// end inline asm
	add.s32 	%r802, %r24, %r8;
	shl.b32 	%r803, %r802, 1;
	add.s32 	%r804, %r797, %r803;
	add.s32 	%r347, %r804, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r379, %r380, %r475, %r476}, [%r347];
	// end inline asm
	add.s32 	%r352, %r797, %r805;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r571, %r572, %r667, %r668}, [%r352];
	// end inline asm
	add.s32 	%r357, %r804, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r385, %r386, %r481, %r482}, [%r357];
	// end inline asm
	add.s32 	%r362, %r797, %r806;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r577, %r578, %r673, %r674}, [%r362];
	// end inline asm
	.loc	1 78 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:78:25
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f642, %f643, %f644, %f645 }, { %r363, %r364, %r365, %r366 }, { %r367, %r368 }, { %f642, %f643, %f644, %f645 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f646, %f647, %f648, %f649 }, { %r363, %r364, %r365, %r366 }, { %r373, %r374 }, { %f646, %f647, %f648, %f649 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f650, %f651, %f652, %f653 }, { %r363, %r364, %r365, %r366 }, { %r379, %r380 }, { %f650, %f651, %f652, %f653 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f654, %f655, %f656, %f657 }, { %r363, %r364, %r365, %r366 }, { %r385, %r386 }, { %f654, %f655, %f656, %f657 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f658, %f659, %f660, %f661 }, { %r387, %r388, %r389, %r390 }, { %r367, %r368 }, { %f658, %f659, %f660, %f661 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f662, %f663, %f664, %f665 }, { %r387, %r388, %r389, %r390 }, { %r373, %r374 }, { %f662, %f663, %f664, %f665 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f666, %f667, %f668, %f669 }, { %r387, %r388, %r389, %r390 }, { %r379, %r380 }, { %f666, %f667, %f668, %f669 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f670, %f671, %f672, %f673 }, { %r387, %r388, %r389, %r390 }, { %r385, %r386 }, { %f670, %f671, %f672, %f673 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f674, %f675, %f676, %f677 }, { %r411, %r412, %r413, %r414 }, { %r367, %r368 }, { %f674, %f675, %f676, %f677 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f678, %f679, %f680, %f681 }, { %r411, %r412, %r413, %r414 }, { %r373, %r374 }, { %f678, %f679, %f680, %f681 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f682, %f683, %f684, %f685 }, { %r411, %r412, %r413, %r414 }, { %r379, %r380 }, { %f682, %f683, %f684, %f685 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f686, %f687, %f688, %f689 }, { %r411, %r412, %r413, %r414 }, { %r385, %r386 }, { %f686, %f687, %f688, %f689 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f690, %f691, %f692, %f693 }, { %r435, %r436, %r437, %r438 }, { %r367, %r368 }, { %f690, %f691, %f692, %f693 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f694, %f695, %f696, %f697 }, { %r435, %r436, %r437, %r438 }, { %r373, %r374 }, { %f694, %f695, %f696, %f697 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f698, %f699, %f700, %f701 }, { %r435, %r436, %r437, %r438 }, { %r379, %r380 }, { %f698, %f699, %f700, %f701 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f702, %f703, %f704, %f705 }, { %r435, %r436, %r437, %r438 }, { %r385, %r386 }, { %f702, %f703, %f704, %f705 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f642, %f643, %f644, %f645 }, { %r459, %r460, %r461, %r462 }, { %r463, %r464 }, { %f642, %f643, %f644, %f645 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f646, %f647, %f648, %f649 }, { %r459, %r460, %r461, %r462 }, { %r469, %r470 }, { %f646, %f647, %f648, %f649 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f650, %f651, %f652, %f653 }, { %r459, %r460, %r461, %r462 }, { %r475, %r476 }, { %f650, %f651, %f652, %f653 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f654, %f655, %f656, %f657 }, { %r459, %r460, %r461, %r462 }, { %r481, %r482 }, { %f654, %f655, %f656, %f657 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f658, %f659, %f660, %f661 }, { %r483, %r484, %r485, %r486 }, { %r463, %r464 }, { %f658, %f659, %f660, %f661 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f662, %f663, %f664, %f665 }, { %r483, %r484, %r485, %r486 }, { %r469, %r470 }, { %f662, %f663, %f664, %f665 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f666, %f667, %f668, %f669 }, { %r483, %r484, %r485, %r486 }, { %r475, %r476 }, { %f666, %f667, %f668, %f669 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f670, %f671, %f672, %f673 }, { %r483, %r484, %r485, %r486 }, { %r481, %r482 }, { %f670, %f671, %f672, %f673 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f674, %f675, %f676, %f677 }, { %r507, %r508, %r509, %r510 }, { %r463, %r464 }, { %f674, %f675, %f676, %f677 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f678, %f679, %f680, %f681 }, { %r507, %r508, %r509, %r510 }, { %r469, %r470 }, { %f678, %f679, %f680, %f681 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f682, %f683, %f684, %f685 }, { %r507, %r508, %r509, %r510 }, { %r475, %r476 }, { %f682, %f683, %f684, %f685 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f686, %f687, %f688, %f689 }, { %r507, %r508, %r509, %r510 }, { %r481, %r482 }, { %f686, %f687, %f688, %f689 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f690, %f691, %f692, %f693 }, { %r531, %r532, %r533, %r534 }, { %r463, %r464 }, { %f690, %f691, %f692, %f693 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f694, %f695, %f696, %f697 }, { %r531, %r532, %r533, %r534 }, { %r469, %r470 }, { %f694, %f695, %f696, %f697 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f698, %f699, %f700, %f701 }, { %r531, %r532, %r533, %r534 }, { %r475, %r476 }, { %f698, %f699, %f700, %f701 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f702, %f703, %f704, %f705 }, { %r531, %r532, %r533, %r534 }, { %r481, %r482 }, { %f702, %f703, %f704, %f705 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f642, %f643, %f644, %f645 }, { %r555, %r556, %r557, %r558 }, { %r559, %r560 }, { %f642, %f643, %f644, %f645 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f646, %f647, %f648, %f649 }, { %r555, %r556, %r557, %r558 }, { %r565, %r566 }, { %f646, %f647, %f648, %f649 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f650, %f651, %f652, %f653 }, { %r555, %r556, %r557, %r558 }, { %r571, %r572 }, { %f650, %f651, %f652, %f653 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f654, %f655, %f656, %f657 }, { %r555, %r556, %r557, %r558 }, { %r577, %r578 }, { %f654, %f655, %f656, %f657 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f658, %f659, %f660, %f661 }, { %r579, %r580, %r581, %r582 }, { %r559, %r560 }, { %f658, %f659, %f660, %f661 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f662, %f663, %f664, %f665 }, { %r579, %r580, %r581, %r582 }, { %r565, %r566 }, { %f662, %f663, %f664, %f665 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f666, %f667, %f668, %f669 }, { %r579, %r580, %r581, %r582 }, { %r571, %r572 }, { %f666, %f667, %f668, %f669 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f670, %f671, %f672, %f673 }, { %r579, %r580, %r581, %r582 }, { %r577, %r578 }, { %f670, %f671, %f672, %f673 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f674, %f675, %f676, %f677 }, { %r603, %r604, %r605, %r606 }, { %r559, %r560 }, { %f674, %f675, %f676, %f677 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f678, %f679, %f680, %f681 }, { %r603, %r604, %r605, %r606 }, { %r565, %r566 }, { %f678, %f679, %f680, %f681 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f682, %f683, %f684, %f685 }, { %r603, %r604, %r605, %r606 }, { %r571, %r572 }, { %f682, %f683, %f684, %f685 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f686, %f687, %f688, %f689 }, { %r603, %r604, %r605, %r606 }, { %r577, %r578 }, { %f686, %f687, %f688, %f689 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f690, %f691, %f692, %f693 }, { %r627, %r628, %r629, %r630 }, { %r559, %r560 }, { %f690, %f691, %f692, %f693 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f694, %f695, %f696, %f697 }, { %r627, %r628, %r629, %r630 }, { %r565, %r566 }, { %f694, %f695, %f696, %f697 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f698, %f699, %f700, %f701 }, { %r627, %r628, %r629, %r630 }, { %r571, %r572 }, { %f698, %f699, %f700, %f701 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f702, %f703, %f704, %f705 }, { %r627, %r628, %r629, %r630 }, { %r577, %r578 }, { %f702, %f703, %f704, %f705 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f642, %f643, %f644, %f645 }, { %r651, %r652, %r653, %r654 }, { %r655, %r656 }, { %f642, %f643, %f644, %f645 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f646, %f647, %f648, %f649 }, { %r651, %r652, %r653, %r654 }, { %r661, %r662 }, { %f646, %f647, %f648, %f649 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f650, %f651, %f652, %f653 }, { %r651, %r652, %r653, %r654 }, { %r667, %r668 }, { %f650, %f651, %f652, %f653 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f654, %f655, %f656, %f657 }, { %r651, %r652, %r653, %r654 }, { %r673, %r674 }, { %f654, %f655, %f656, %f657 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f658, %f659, %f660, %f661 }, { %r675, %r676, %r677, %r678 }, { %r655, %r656 }, { %f658, %f659, %f660, %f661 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f662, %f663, %f664, %f665 }, { %r675, %r676, %r677, %r678 }, { %r661, %r662 }, { %f662, %f663, %f664, %f665 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f666, %f667, %f668, %f669 }, { %r675, %r676, %r677, %r678 }, { %r667, %r668 }, { %f666, %f667, %f668, %f669 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f670, %f671, %f672, %f673 }, { %r675, %r676, %r677, %r678 }, { %r673, %r674 }, { %f670, %f671, %f672, %f673 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f674, %f675, %f676, %f677 }, { %r699, %r700, %r701, %r702 }, { %r655, %r656 }, { %f674, %f675, %f676, %f677 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f678, %f679, %f680, %f681 }, { %r699, %r700, %r701, %r702 }, { %r661, %r662 }, { %f678, %f679, %f680, %f681 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f682, %f683, %f684, %f685 }, { %r699, %r700, %r701, %r702 }, { %r667, %r668 }, { %f682, %f683, %f684, %f685 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f686, %f687, %f688, %f689 }, { %r699, %r700, %r701, %r702 }, { %r673, %r674 }, { %f686, %f687, %f688, %f689 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f690, %f691, %f692, %f693 }, { %r723, %r724, %r725, %r726 }, { %r655, %r656 }, { %f690, %f691, %f692, %f693 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f694, %f695, %f696, %f697 }, { %r723, %r724, %r725, %r726 }, { %r661, %r662 }, { %f694, %f695, %f696, %f697 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f698, %f699, %f700, %f701 }, { %r723, %r724, %r725, %r726 }, { %r667, %r668 }, { %f698, %f699, %f700, %f701 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f702, %f703, %f704, %f705 }, { %r723, %r724, %r725, %r726 }, { %r673, %r674 }, { %f702, %f703, %f704, %f705 };
	// end inline asm
	.loc	1 64 26                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:64:26
	add.s32 	%r807, %r929, 1;
	setp.lt.s32 	%p3, %r807, 2;
	selp.b32 	%r929, %r807, 0, %p3;
	.loc	1 72 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:25
	add.s64 	%rd143, %rd12, %rd171;
	add.s64 	%rd144, %rd11, %rd171;
	add.s64 	%rd145, %rd10, %rd171;
	.loc	1 72 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:72:20
	add.s64 	%rd146, %rd9, %rd171;
	shl.b32 	%r808, %r929, 13;
	add.s32 	%r809, %r774, %r808;
	bar.sync 	0;
	add.s32 	%r747, %r809, %r210;
	add.s32 	%r749, %r747, 2048;
	add.s32 	%r751, %r747, 4096;
	add.s32 	%r753, %r747, 6144;
	selp.b32 	%r748, 16, 0, %p1;
	// begin inline asm
	cp.async.cg.shared.global [ %r747 + 0 ], [ %rd143 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r749 + 0 ], [ %rd144 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r751 + 0 ], [ %rd145 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r753 + 0 ], [ %rd146 + 0 ], 0x10, %r748;
	// end inline asm
	cp.async.commit_group;
	.loc	1 77 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:25
	add.s64 	%rd147, %rd8, %rd171;
	add.s64 	%rd148, %rd7, %rd171;
	add.s64 	%rd149, %rd6, %rd171;
	add.s64 	%rd150, %rd5, %rd171;
	add.s64 	%rd151, %rd4, %rd171;
	add.s64 	%rd152, %rd3, %rd171;
	add.s64 	%rd153, %rd2, %rd171;
	.loc	1 77 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:77:20
	add.s64 	%rd154, %rd1, %rd171;
	shl.b32 	%r811, %r929, 14;
	add.s32 	%r812, %r211, %r811;
	add.s32 	%r755, %r812, %r210;
	add.s32 	%r757, %r755, 2048;
	add.s32 	%r759, %r755, 4096;
	add.s32 	%r761, %r755, 6144;
	add.s32 	%r763, %r755, 8192;
	add.s32 	%r765, %r755, 10240;
	add.s32 	%r767, %r755, 12288;
	add.s32 	%r769, %r755, 14336;
	// begin inline asm
	cp.async.cg.shared.global [ %r755 + 0 ], [ %rd147 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r757 + 0 ], [ %rd148 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r759 + 0 ], [ %rd149 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r761 + 0 ], [ %rd150 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r763 + 0 ], [ %rd151 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r765 + 0 ], [ %rd152 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r767 + 0 ], [ %rd153 + 0 ], 0x10, %r748;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r769 + 0 ], [ %rd154 + 0 ], 0x10, %r748;
	// end inline asm
	cp.async.commit_group;
	.loc	1 64 26                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:64:26
	add.s64 	%rd172, %rd172, 1;
	add.s64 	%rd171, %rd171, 128;
	setp.ne.s64 	%p4, %rd171, 1536;
	@%p4 bra 	$L__BB0_1;
// %bb.2:
	.loc	1 52 40                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:52:40
	shl.b32 	%r877, %r3, 3;
	or.b32  	%r878, %r7, %r877;
	.loc	1 52 27                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:52:27
	or.b32  	%r879, %r5, %r878;
	.loc	1 51 40                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:51:40
	bfe.u32 	%r880, %r2, 4, 3;
	.loc	1 51 27                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:51:27
	or.b32  	%r881, %r880, %r1;
	or.b32  	%r882, %r881, 56;
	or.b32  	%r883, %r881, 48;
	or.b32  	%r884, %r881, 40;
	or.b32  	%r885, %r881, 32;
	or.b32  	%r886, %r881, 24;
	or.b32  	%r887, %r881, 16;
	or.b32  	%r888, %r881, 8;
	.loc	1 64 26                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:64:26
	cp.async.wait_group 0;
	bar.sync 	0;
	.loc	1 85 20                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:85:20
	setp.lt.s32 	%p45, %r881, 77;
	setp.lt.s32 	%p46, %r888, 77;
	setp.lt.s32 	%p47, %r887, 77;
	setp.lt.s32 	%p48, %r886, 77;
	setp.lt.s32 	%p49, %r885, 77;
	setp.lt.s32 	%p50, %r884, 77;
	setp.lt.s32 	%p51, %r883, 77;
	setp.lt.s32 	%p52, %r882, 77;
	.loc	1 85 34                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:85:34
	setp.lt.s32 	%p53, %r879, 768;
	.loc	1 85 26                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:85:26
	and.pred  	%p37, %p45, %p53;
	and.pred  	%p38, %p46, %p53;
	and.pred  	%p39, %p47, %p53;
	and.pred  	%p40, %p48, %p53;
	and.pred  	%p41, %p49, %p53;
	and.pred  	%p42, %p50, %p53;
	and.pred  	%p43, %p51, %p53;
	and.pred  	%p44, %p52, %p53;
	.loc	1 88 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:88:25
	mad.lo.s32 	%r889, %r881, 768, %r879;
	add.s32 	%r890, %r889, 6144;
	add.s32 	%r891, %r889, 12288;
	add.s32 	%r892, %r889, 18432;
	add.s32 	%r893, %r889, 24576;
	add.s32 	%r894, %r889, 30720;
	add.s32 	%r895, %r889, 36864;
	.loc	1 88 21                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:88:21
	add.s32 	%r896, %r889, 43008;
	.loc	1 89 25                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:89:25
	mul.wide.s32 	%rd163, %r889, 2;
	add.s64 	%rd155, %rd17, %rd163;
	mul.wide.s32 	%rd164, %r890, 2;
	add.s64 	%rd156, %rd17, %rd164;
	mul.wide.s32 	%rd165, %r891, 2;
	add.s64 	%rd157, %rd17, %rd165;
	mul.wide.s32 	%rd166, %r892, 2;
	add.s64 	%rd158, %rd17, %rd166;
	mul.wide.s32 	%rd167, %r893, 2;
	add.s64 	%rd159, %rd17, %rd167;
	mul.wide.s32 	%rd168, %r894, 2;
	add.s64 	%rd160, %rd17, %rd168;
	mul.wide.s32 	%rd169, %r895, 2;
	add.s64 	%rd161, %rd17, %rd169;
	mul.wide.s32 	%rd170, %r896, 2;
	add.s64 	%rd162, %rd17, %rd170;
	.loc	1 89 67                         // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:89:67
	cvt.rn.bf16.f32 	%rs1, %f642;
	cvt.rn.bf16.f32 	%rs2, %f643;
	cvt.rn.bf16.f32 	%rs3, %f644;
	cvt.rn.bf16.f32 	%rs4, %f645;
	cvt.rn.bf16.f32 	%rs5, %f646;
	cvt.rn.bf16.f32 	%rs6, %f647;
	cvt.rn.bf16.f32 	%rs7, %f648;
	cvt.rn.bf16.f32 	%rs8, %f649;
	cvt.rn.bf16.f32 	%rs9, %f650;
	cvt.rn.bf16.f32 	%rs10, %f651;
	cvt.rn.bf16.f32 	%rs11, %f652;
	cvt.rn.bf16.f32 	%rs12, %f653;
	cvt.rn.bf16.f32 	%rs13, %f654;
	cvt.rn.bf16.f32 	%rs14, %f655;
	cvt.rn.bf16.f32 	%rs15, %f656;
	cvt.rn.bf16.f32 	%rs16, %f657;
	cvt.rn.bf16.f32 	%rs17, %f658;
	cvt.rn.bf16.f32 	%rs18, %f659;
	cvt.rn.bf16.f32 	%rs19, %f660;
	cvt.rn.bf16.f32 	%rs20, %f661;
	cvt.rn.bf16.f32 	%rs21, %f662;
	cvt.rn.bf16.f32 	%rs22, %f663;
	cvt.rn.bf16.f32 	%rs23, %f664;
	cvt.rn.bf16.f32 	%rs24, %f665;
	cvt.rn.bf16.f32 	%rs25, %f666;
	cvt.rn.bf16.f32 	%rs26, %f667;
	cvt.rn.bf16.f32 	%rs27, %f668;
	cvt.rn.bf16.f32 	%rs28, %f669;
	cvt.rn.bf16.f32 	%rs29, %f670;
	cvt.rn.bf16.f32 	%rs30, %f671;
	cvt.rn.bf16.f32 	%rs31, %f672;
	cvt.rn.bf16.f32 	%rs32, %f673;
	cvt.rn.bf16.f32 	%rs33, %f674;
	cvt.rn.bf16.f32 	%rs34, %f675;
	cvt.rn.bf16.f32 	%rs35, %f676;
	cvt.rn.bf16.f32 	%rs36, %f677;
	cvt.rn.bf16.f32 	%rs37, %f678;
	cvt.rn.bf16.f32 	%rs38, %f679;
	cvt.rn.bf16.f32 	%rs39, %f680;
	cvt.rn.bf16.f32 	%rs40, %f681;
	cvt.rn.bf16.f32 	%rs41, %f682;
	cvt.rn.bf16.f32 	%rs42, %f683;
	cvt.rn.bf16.f32 	%rs43, %f684;
	cvt.rn.bf16.f32 	%rs44, %f685;
	cvt.rn.bf16.f32 	%rs45, %f686;
	cvt.rn.bf16.f32 	%rs46, %f687;
	cvt.rn.bf16.f32 	%rs47, %f688;
	cvt.rn.bf16.f32 	%rs48, %f689;
	cvt.rn.bf16.f32 	%rs49, %f690;
	cvt.rn.bf16.f32 	%rs50, %f691;
	cvt.rn.bf16.f32 	%rs51, %f692;
	cvt.rn.bf16.f32 	%rs52, %f693;
	cvt.rn.bf16.f32 	%rs53, %f694;
	cvt.rn.bf16.f32 	%rs54, %f695;
	cvt.rn.bf16.f32 	%rs55, %f696;
	cvt.rn.bf16.f32 	%rs56, %f697;
	cvt.rn.bf16.f32 	%rs57, %f698;
	cvt.rn.bf16.f32 	%rs58, %f699;
	cvt.rn.bf16.f32 	%rs59, %f700;
	cvt.rn.bf16.f32 	%rs60, %f701;
	cvt.rn.bf16.f32 	%rs61, %f702;
	cvt.rn.bf16.f32 	%rs62, %f703;
	cvt.rn.bf16.f32 	%rs63, %f704;
	cvt.rn.bf16.f32 	%rs64, %f705;
	shl.b32 	%r897, %r2, 1;
	and.b32  	%r898, %r897, 6;
	shl.b32 	%r899, %r2, 5;
	and.b32  	%r900, %r899, 384;
	or.b32  	%r901, %r900, %r898;
	shl.b32 	%r902, %r4, 5;
	or.b32  	%r903, %r901, %r902;
	and.b32  	%r904, %r23, 24;
	or.b32  	%r905, %r903, %r904;
	and.b32  	%r906, %r6, 1016;
	shr.u32 	%r907, %r903, 4;
	add.s32 	%r908, %r907, %r905;
	shl.b32 	%r909, %r908, 1;
	add.s32 	%r813, %r211, %r909;
	mov.pred 	%p5, -1;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r813 + 0 ], { %rs1, %rs2 };
	// end inline asm
	or.b32  	%r911, %r903, 1024;
	shr.u32 	%r912, %r911, 4;
	and.b32  	%r913, %r912, 120;
	add.s32 	%r914, %r913, %r905;
	shl.b32 	%r915, %r914, 1;
	add.s32 	%r916, %r211, %r915;
	add.s32 	%r814, %r916, 2048;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r814 + 0 ], { %rs3, %rs4 };
	// end inline asm
	add.s32 	%r815, %r813, 64;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r815 + 0 ], { %rs5, %rs6 };
	// end inline asm
	add.s32 	%r816, %r916, 2112;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r816 + 0 ], { %rs7, %rs8 };
	// end inline asm
	add.s32 	%r817, %r813, 128;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r817 + 0 ], { %rs9, %rs10 };
	// end inline asm
	add.s32 	%r818, %r916, 2176;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r818 + 0 ], { %rs11, %rs12 };
	// end inline asm
	add.s32 	%r819, %r813, 192;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r819 + 0 ], { %rs13, %rs14 };
	// end inline asm
	add.s32 	%r820, %r916, 2240;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r820 + 0 ], { %rs15, %rs16 };
	// end inline asm
	bar.sync 	0;
	shr.u32 	%r917, %r6, 4;
	and.b32  	%r918, %r917, 56;
	add.s32 	%r919, %r918, %r906;
	shl.b32 	%r920, %r919, 1;
	add.s32 	%r921, %r211, %r920;
	ld.shared.v4.u32 	{%r845, %r846, %r847, %r848}, [%r921];
	or.b32  	%r922, %r906, 1024;
	shr.u32 	%r923, %r922, 4;
	and.b32  	%r924, %r923, 120;
	add.s32 	%r925, %r924, %r906;
	shl.b32 	%r926, %r925, 1;
	add.s32 	%r927, %r211, %r926;
	ld.shared.v4.u32 	{%r849, %r850, %r851, %r852}, [%r927+2048];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r813 + 0 ], { %rs17, %rs18 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r814 + 0 ], { %rs19, %rs20 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r815 + 0 ], { %rs21, %rs22 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r816 + 0 ], { %rs23, %rs24 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r817 + 0 ], { %rs25, %rs26 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r818 + 0 ], { %rs27, %rs28 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r819 + 0 ], { %rs29, %rs30 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r820 + 0 ], { %rs31, %rs32 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r853, %r854, %r855, %r856}, [%r921];
	ld.shared.v4.u32 	{%r857, %r858, %r859, %r860}, [%r927+2048];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r813 + 0 ], { %rs33, %rs34 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r814 + 0 ], { %rs35, %rs36 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r815 + 0 ], { %rs37, %rs38 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r816 + 0 ], { %rs39, %rs40 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r817 + 0 ], { %rs41, %rs42 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r818 + 0 ], { %rs43, %rs44 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r819 + 0 ], { %rs45, %rs46 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r820 + 0 ], { %rs47, %rs48 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r861, %r862, %r863, %r864}, [%r921];
	ld.shared.v4.u32 	{%r865, %r866, %r867, %r868}, [%r927+2048];
	bar.sync 	0;
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r813 + 0 ], { %rs49, %rs50 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r814 + 0 ], { %rs51, %rs52 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r815 + 0 ], { %rs53, %rs54 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r816 + 0 ], { %rs55, %rs56 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r817 + 0 ], { %rs57, %rs58 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r818 + 0 ], { %rs59, %rs60 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r819 + 0 ], { %rs61, %rs62 };
	// end inline asm
	// begin inline asm
	@%p5 st.shared.v2.b16 [ %r820 + 0 ], { %rs63, %rs64 };
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r869, %r870, %r871, %r872}, [%r921];
	ld.shared.v4.u32 	{%r873, %r874, %r875, %r876}, [%r927+2048];
	// begin inline asm
	@%p37 st.global.v4.b32 [ %rd155 + 0 ], { %r845, %r846, %r847, %r848 };
	// end inline asm
	// begin inline asm
	@%p38 st.global.v4.b32 [ %rd156 + 0 ], { %r849, %r850, %r851, %r852 };
	// end inline asm
	// begin inline asm
	@%p39 st.global.v4.b32 [ %rd157 + 0 ], { %r853, %r854, %r855, %r856 };
	// end inline asm
	// begin inline asm
	@%p40 st.global.v4.b32 [ %rd158 + 0 ], { %r857, %r858, %r859, %r860 };
	// end inline asm
	// begin inline asm
	@%p41 st.global.v4.b32 [ %rd159 + 0 ], { %r861, %r862, %r863, %r864 };
	// end inline asm
	// begin inline asm
	@%p42 st.global.v4.b32 [ %rd160 + 0 ], { %r865, %r866, %r867, %r868 };
	// end inline asm
	// begin inline asm
	@%p43 st.global.v4.b32 [ %rd161 + 0 ], { %r869, %r870, %r871, %r872 };
	// end inline asm
	// begin inline asm
	@%p44 st.global.v4.b32 [ %rd162 + 0 ], { %r873, %r874, %r875, %r876 };
	// end inline asm
	.loc	1 89 4                          // cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py:89:4
	ret;
$L__tmp1:
$L__func_end0:
                                        // -- End function
}
	.file	1 "/tmp/torchinductor_root/xz/cxz5modueljzda62rtoqgawwiirmmtrwgwdf7u64m4jxelvlpmzb.py"
	.section	.debug_abbrev
	{
.b8 1                                   // Abbreviation Code
.b8 17                                  // DW_TAG_compile_unit
.b8 0                                   // DW_CHILDREN_no
.b8 37                                  // DW_AT_producer
.b8 8                                   // DW_FORM_string
.b8 19                                  // DW_AT_language
.b8 5                                   // DW_FORM_data2
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 16                                  // DW_AT_stmt_list
.b8 6                                   // DW_FORM_data4
.b8 27                                  // DW_AT_comp_dir
.b8 8                                   // DW_FORM_string
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 0                                   // EOM(3)
	}
	.section	.debug_info
	{
.b32 104                                // Length of Unit
.b8 2                                   // DWARF version number
.b8 0
.b32 .debug_abbrev                      // Offset Into Abbrev. Section
.b8 8                                   // Address Size (in bytes)
.b8 1                                   // Abbrev [1] 0xb:0x61 DW_TAG_compile_unit
.b8 116                                 // DW_AT_producer
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2                                   // DW_AT_language
.b8 0
.b8 99                                  // DW_AT_name
.b8 120
.b8 122
.b8 53
.b8 109
.b8 111
.b8 100
.b8 117
.b8 101
.b8 108
.b8 106
.b8 122
.b8 100
.b8 97
.b8 54
.b8 50
.b8 114
.b8 116
.b8 111
.b8 113
.b8 103
.b8 97
.b8 119
.b8 119
.b8 105
.b8 105
.b8 114
.b8 109
.b8 109
.b8 116
.b8 114
.b8 119
.b8 103
.b8 119
.b8 100
.b8 102
.b8 55
.b8 117
.b8 54
.b8 52
.b8 109
.b8 52
.b8 106
.b8 120
.b8 101
.b8 108
.b8 118
.b8 108
.b8 112
.b8 109
.b8 122
.b8 98
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line                        // DW_AT_stmt_list
.b8 47                                  // DW_AT_comp_dir
.b8 116
.b8 109
.b8 112
.b8 47
.b8 116
.b8 111
.b8 114
.b8 99
.b8 104
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 114
.b8 111
.b8 111
.b8 116
.b8 47
.b8 120
.b8 122
.b8 0
	}
	.section	.debug_macinfo	{	}
